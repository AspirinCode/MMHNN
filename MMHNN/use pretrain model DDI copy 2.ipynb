{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371 177\n"
     ]
    }
   ],
   "source": [
    "#导入模型参数\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"not removing hydrogen atom without neighbors\")\n",
    "import sys\n",
    "sys.path.append(\"/home/dwj/WWW/DDIsubgraph/pretrain\")\n",
    "from dataset.dataset import MoleculeDatasetWrapper\n",
    "from models.model_new import Model001\n",
    "from loss_utils.nt_xent import NTXentLoss\n",
    "from loss_utils.weighted_nt_xent import Weighted_NTXentLoss\n",
    "from loss_utils.motif_loss import Motif_Loss\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=1,\n",
    "                    help='which gpu to use if any (default: 0)')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help=' inputbatch size for training (default: 1024)')\n",
    "parser.add_argument('--dataset', type=str, default='/home/dwj/WWW/DDIsubgraph/pretrain/data/Compsol.csv',\n",
    "                    help='root directory of dataset.')\n",
    "\n",
    "parser.add_argument('--valid_size', type=float, default=0.05,\n",
    "                    help='valid_size (default: 0.2)')\n",
    "parser.add_argument('--num_workers', type=int, default=0,\n",
    "                    help=' the number of workers to load data (default: 8)')\n",
    "\n",
    "\n",
    "parser.add_argument('--num_layer', type=int, default=4,\n",
    "                    help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                    help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--feat_dim', type=int, default=300,\n",
    "                    help='embedding dimensions (default: 256)')\n",
    "parser.add_argument('--dropout_gin', type=float, default=0,\n",
    "                    help='dropout ratio (default: 0.2)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                    help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--N', type=int, default=2,\n",
    "                    help='num layer of transformer encoder')\n",
    "parser.add_argument('--d_model', type=int, default=256,\n",
    "                    help='embedding dimensions (default: 256)')\n",
    "parser.add_argument('--d_ff', type=int, default=1024,\n",
    "                    help='embedding dimensions (default: 1024)')\n",
    "parser.add_argument('--h', type=int, default=8,\n",
    "                    help='heads of transformer encoder(default: 8)')\n",
    "parser.add_argument('--dropout_encoder', type=float, default=0.1,\n",
    "                    help='dropout ratio (default: 0.1)')\n",
    "parser.add_argument('--weight', type=int, default=1,\n",
    "                    help='weight or not')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "\n",
    "parser.add_argument('--decay', type=float, default=0.00001,\n",
    "                    help='weight decay (default: 0)')\n",
    "parser.add_argument('--lr', type=float, default=0.0005,\n",
    "                    help='learning rate (default: 0.001)')\n",
    "\n",
    "\n",
    "parser.add_argument('--output_model_file', type=str, default='/home/dwj/WWW/DDIsubgraph/pretrain/save_model/pretrain/motif_loss+weight',\n",
    "                    help='filename to output the pre-trained model')\n",
    "parser.add_argument('--gama', type=float, default=0.2, help='weight of motif_loss')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "dataset = MoleculeDatasetWrapper(args.batch_size, args.num_workers, args.valid_size, args.dataset)   # dataset：一个txt文件路径，里面村的是smiles\n",
    "train_loader, valid_loader = dataset.get_data_loaders()  # 导入数据集，编码，在dataset里\n",
    "model = Model001(num_layer=args.num_layer, emb_dim=args.emb_dim, feat_dim=args.feat_dim,\n",
    "                 dropout_gin=args.dropout_gin, pool=args.graph_pooling, device=device,\n",
    "                 N=args.N, d_model=args.d_model, d_ff=args.d_ff, h=args.h,\n",
    "                 dropout_encoder=args.dropout_encoder).to(device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a40b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f71e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "import layers1\n",
    "import models1\n",
    "import custom_loss\n",
    "import time\n",
    "import torch.nn as nn\n",
    "#'C','N','O', 'S','F','Si','P', 'Cl','Br','Mg','Na','Ca','Fe','As','Al','I','B','V','K','Tl',\n",
    "#            'Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn','H', 'Li','Ge','Cu','Au','Ni','Cd','In',\n",
    "#            'Mn','Zr','Cr','Pt','Hg','Pb','Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f80ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_atom_feats = 55\n",
    "n_atom_hid = 256\n",
    "rel_total = 86\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 300\n",
    "neg_samples = 1\n",
    "batch_size = 128\n",
    "data_size_ratio = 1\n",
    "kge_dim = 384\n",
    "beta = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbcd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(pred, target):\n",
    "    labels_tensor = torch.tensor([float(x) for x in target], dtype=torch.float32).unsqueeze(1).to(pred.device)\n",
    "    return F.mse_loss(pred, labels_tensor )\n",
    "def mae_loss(pred, target):\n",
    "    # 将目标值转换为浮点数张量，并调整形状以匹配预测值\n",
    "    labels_tensor = torch.tensor([float(x) for x in target], dtype=torch.float32).unsqueeze(1).to(pred.device)\n",
    "    # 计算绝对误差\n",
    "    absolute_errors = torch.abs(pred - labels_tensor)\n",
    "    # 返回平均绝对误差\n",
    "    return torch.mean(absolute_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737fa54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, val_data_loader, loss_fn,  optimizer, n_epochs, device, scheduler=None):\n",
    "    print('Starting training at', datetime.today())\n",
    "    for i in range(1, n_epochs+1):\n",
    "        start = time.time()\n",
    "        train_loss = 0\n",
    "        train_loss_pos = 0\n",
    "        train_loss_neg = 0\n",
    "        val_loss = 0\n",
    "        val_loss_pos = 0\n",
    "        val_loss_neg = 0\n",
    "\n",
    "        sum1=0\n",
    "        for batch in train_data_loader:\n",
    "            model.train()\n",
    "            p_score,p_score_gib,KL_loss = model(batch,device)\n",
    "            loss= rmse_loss(p_score,batch[2])\n",
    "            # print('orgin loss : ',loss)\n",
    "            loss_gib= rmse_loss(p_score_gib,batch[2])\n",
    "            # print('gib loss : ',loss_gib)\n",
    "            loss += loss_gib\n",
    "            # print('KL_loss : ',KL_loss)\n",
    "            # loss += beta * KL_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum1+=len(batch[2]) \n",
    "            train_mae = mae_loss(p_score,batch[2])\n",
    "            train_mae += train_mae.item() * len(p_score)\n",
    "            train_rmse = rmse_loss(p_score,batch[2])\n",
    "            train_rmse += train_rmse.item() * len(p_score)\n",
    "        train_mae /= sum1\n",
    "        train_rmse /= sum1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sum2=0\n",
    "            for batch in val_data_loader:\n",
    "                model.eval()\n",
    "                p_score,_,_ = model(batch,device)\n",
    "                loss= rmse_loss(p_score,batch[2]) \n",
    "                #print(len(batch[2])  ) \n",
    "                \n",
    "                sum2+=len(batch[2])\n",
    "                val_mae = mae_loss(p_score,batch[2])       \n",
    "                val_mae += val_mae.item() * len(p_score)\n",
    "                val_rmse = rmse_loss(p_score,batch[2])       \n",
    "                val_rmse += val_rmse.item() * len(p_score)\n",
    "            val_mae /= sum2\n",
    "            val_rmse /= sum2\n",
    "            \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"epoch {i} train_mae:  {train_mae} and val_mae:  {val_mae}  train_rmse:  {train_rmse} and val_rmse:  {val_rmse}/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ff474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = custom_loss.SigmoidLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.96 ** (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d803ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2024-09-05 19:09:10.711037\n",
      "epoch 1 train_mae:  0.12294919788837433 and val_mae:  4.197122097015381  train_rmse:  0.4647532105445862 and val_rmse:  20.798416137695312/n\n",
      "epoch 2 train_mae:  0.08844417333602905 and val_mae:  3.6892144680023193  train_rmse:  0.2789400517940521 and val_rmse:  15.833293914794922/n\n",
      "epoch 3 train_mae:  0.07619231939315796 and val_mae:  3.289503812789917  train_rmse:  0.23352760076522827 and val_rmse:  13.378849983215332/n\n",
      "epoch 4 train_mae:  0.062901571393013 and val_mae:  1.7181638479232788  train_rmse:  0.17345625162124634 and val_rmse:  4.5103302001953125/n\n",
      "epoch 5 train_mae:  0.04908803850412369 and val_mae:  2.5175132751464844  train_rmse:  0.09803038090467453 and val_rmse:  8.491049766540527/n\n",
      "epoch 6 train_mae:  0.04589950293302536 and val_mae:  1.48113214969635  train_rmse:  0.08161463588476181 and val_rmse:  3.4783332347869873/n\n",
      "epoch 7 train_mae:  0.040843576192855835 and val_mae:  1.025793433189392  train_rmse:  0.06951577216386795 and val_rmse:  2.162069797515869/n\n",
      "epoch 8 train_mae:  0.037849415093660355 and val_mae:  0.9873133301734924  train_rmse:  0.06400386989116669 and val_rmse:  2.1045963764190674/n\n",
      "epoch 9 train_mae:  0.03713216260075569 and val_mae:  1.7995991706848145  train_rmse:  0.06638750433921814 and val_rmse:  4.8881096839904785/n\n",
      "epoch 10 train_mae:  0.0351593978703022 and val_mae:  1.2859865427017212  train_rmse:  0.0521797239780426 and val_rmse:  2.931464195251465/n\n",
      "epoch 11 train_mae:  0.03250308334827423 and val_mae:  1.171553373336792  train_rmse:  0.0446329265832901 and val_rmse:  2.475551128387451/n\n",
      "epoch 12 train_mae:  0.03345300629734993 and val_mae:  0.8464042544364929  train_rmse:  0.04821472242474556 and val_rmse:  1.4351460933685303/n\n",
      "epoch 13 train_mae:  0.034430596977472305 and val_mae:  1.482756495475769  train_rmse:  0.04978553205728531 and val_rmse:  3.4924073219299316/n\n",
      "epoch 14 train_mae:  0.03392789512872696 and val_mae:  0.8423901200294495  train_rmse:  0.0557699054479599 and val_rmse:  1.5085573196411133/n\n",
      "epoch 15 train_mae:  0.03332991153001785 and val_mae:  0.7355808615684509  train_rmse:  0.058786265552043915 and val_rmse:  1.2033637762069702/n\n",
      "epoch 16 train_mae:  0.03322784975171089 and val_mae:  0.8288148641586304  train_rmse:  0.04695405066013336 and val_rmse:  1.4259616136550903/n\n",
      "epoch 17 train_mae:  0.029934976249933243 and val_mae:  0.9151618480682373  train_rmse:  0.03921615704894066 and val_rmse:  1.5658180713653564/n\n",
      "epoch 18 train_mae:  0.030348356813192368 and val_mae:  0.7576949596405029  train_rmse:  0.03747899457812309 and val_rmse:  1.1847950220108032/n\n",
      "epoch 19 train_mae:  0.030197899788618088 and val_mae:  0.6876025199890137  train_rmse:  0.03879844397306442 and val_rmse:  1.0837764739990234/n\n",
      "epoch 20 train_mae:  0.030298631638288498 and val_mae:  0.7360495328903198  train_rmse:  0.03729510307312012 and val_rmse:  1.1460859775543213/n\n",
      "epoch 21 train_mae:  0.02970099076628685 and val_mae:  0.7244116067886353  train_rmse:  0.03923995792865753 and val_rmse:  1.0997909307479858/n\n",
      "epoch 22 train_mae:  0.02523960918188095 and val_mae:  0.7388878464698792  train_rmse:  0.02678808756172657 and val_rmse:  1.0807548761367798/n\n",
      "epoch 23 train_mae:  0.03136854246258736 and val_mae:  0.737723171710968  train_rmse:  0.044731512665748596 and val_rmse:  1.0584771633148193/n\n",
      "epoch 24 train_mae:  0.031210841611027718 and val_mae:  0.6177898049354553  train_rmse:  0.040876854211091995 and val_rmse:  0.8007877469062805/n\n",
      "epoch 25 train_mae:  0.03270589932799339 and val_mae:  0.8948811888694763  train_rmse:  0.06658954918384552 and val_rmse:  1.4448878765106201/n\n",
      "epoch 26 train_mae:  0.02862098440527916 and val_mae:  0.9709929823875427  train_rmse:  0.03827274590730667 and val_rmse:  1.617337703704834/n\n",
      "epoch 27 train_mae:  0.047098368406295776 and val_mae:  0.7026636004447937  train_rmse:  0.0769098550081253 and val_rmse:  1.0218538045883179/n\n",
      "epoch 28 train_mae:  0.03212537616491318 and val_mae:  0.8512962460517883  train_rmse:  0.04743525758385658 and val_rmse:  1.3151557445526123/n\n",
      "epoch 29 train_mae:  0.029169198125600815 and val_mae:  0.6784897446632385  train_rmse:  0.03474467992782593 and val_rmse:  0.8412361741065979/n\n",
      "epoch 30 train_mae:  0.02512608841061592 and val_mae:  0.589296817779541  train_rmse:  0.02808069810271263 and val_rmse:  0.7119043469429016/n\n",
      "epoch 31 train_mae:  0.028973504900932312 and val_mae:  0.6207101345062256  train_rmse:  0.034519094973802567 and val_rmse:  0.7880362868309021/n\n",
      "epoch 32 train_mae:  0.02795455791056156 and val_mae:  1.2012394666671753  train_rmse:  0.03650575876235962 and val_rmse:  2.0139212608337402/n\n",
      "epoch 33 train_mae:  0.03450668603181839 and val_mae:  0.5268368721008301  train_rmse:  0.05525963753461838 and val_rmse:  0.5411002039909363/n\n",
      "epoch 34 train_mae:  0.031445033848285675 and val_mae:  0.5176903605461121  train_rmse:  0.03945699334144592 and val_rmse:  0.5938882231712341/n\n",
      "epoch 35 train_mae:  0.028285177424550056 and val_mae:  0.5817039012908936  train_rmse:  0.03321797400712967 and val_rmse:  0.677948534488678/n\n",
      "epoch 36 train_mae:  0.025499965995550156 and val_mae:  0.6436631083488464  train_rmse:  0.030381634831428528 and val_rmse:  0.7944390177726746/n\n",
      "epoch 37 train_mae:  0.0360681377351284 and val_mae:  0.4785696268081665  train_rmse:  0.04755442962050438 and val_rmse:  0.49587979912757874/n\n",
      "epoch 38 train_mae:  0.028691062703728676 and val_mae:  0.7170689702033997  train_rmse:  0.03554021194577217 and val_rmse:  0.8870197534561157/n\n",
      "epoch 39 train_mae:  0.0254377294331789 and val_mae:  0.5562323927879333  train_rmse:  0.027029162272810936 and val_rmse:  0.5856426954269409/n\n",
      "epoch 40 train_mae:  0.023161841556429863 and val_mae:  0.7583986520767212  train_rmse:  0.025594772771000862 and val_rmse:  1.0019654035568237/n\n",
      "epoch 41 train_mae:  0.024264350533485413 and val_mae:  0.7112742066383362  train_rmse:  0.027324797585606575 and val_rmse:  0.9008876085281372/n\n",
      "epoch 42 train_mae:  0.02818041667342186 and val_mae:  0.5140973329544067  train_rmse:  0.039093948900699615 and val_rmse:  0.6119349002838135/n\n",
      "epoch 43 train_mae:  0.02433989755809307 and val_mae:  0.44795405864715576  train_rmse:  0.022779762744903564 and val_rmse:  0.4471881687641144/n\n",
      "epoch 44 train_mae:  0.02687995508313179 and val_mae:  0.5102874040603638  train_rmse:  0.03217213973402977 and val_rmse:  0.5802716016769409/n\n",
      "epoch 45 train_mae:  0.024044429883360863 and val_mae:  0.46638867259025574  train_rmse:  0.0230573583394289 and val_rmse:  0.45741310715675354/n\n",
      "epoch 46 train_mae:  0.02360859513282776 and val_mae:  0.6226282119750977  train_rmse:  0.023343611508607864 and val_rmse:  0.7013985514640808/n\n",
      "epoch 47 train_mae:  0.022319316864013672 and val_mae:  0.5108981728553772  train_rmse:  0.02183946967124939 and val_rmse:  0.5287412405014038/n\n",
      "epoch 48 train_mae:  0.022706834599375725 and val_mae:  0.47122111916542053  train_rmse:  0.02003907598555088 and val_rmse:  0.47533345222473145/n\n",
      "epoch 49 train_mae:  0.021967396140098572 and val_mae:  0.4402676522731781  train_rmse:  0.02135893516242504 and val_rmse:  0.431885689496994/n\n",
      "epoch 50 train_mae:  0.0242108553647995 and val_mae:  0.5981535315513611  train_rmse:  0.027979273349046707 and val_rmse:  0.6230490207672119/n\n",
      "epoch 51 train_mae:  0.021682942286133766 and val_mae:  0.5568439364433289  train_rmse:  0.019046181812882423 and val_rmse:  0.5758494734764099/n\n",
      "epoch 52 train_mae:  0.024117423221468925 and val_mae:  0.8773810267448425  train_rmse:  0.024192877113819122 and val_rmse:  1.1897271871566772/n\n",
      "epoch 53 train_mae:  0.02430097945034504 and val_mae:  0.4616013467311859  train_rmse:  0.02856537513434887 and val_rmse:  0.39541831612586975/n\n",
      "epoch 54 train_mae:  0.022076720371842384 and val_mae:  0.698855996131897  train_rmse:  0.022170113399624825 and val_rmse:  0.7953889966011047/n\n",
      "epoch 55 train_mae:  0.02292587235569954 and val_mae:  0.48062190413475037  train_rmse:  0.02404516562819481 and val_rmse:  0.4330386221408844/n\n",
      "epoch 56 train_mae:  0.022344155237078667 and val_mae:  0.4158501625061035  train_rmse:  0.021864809095859528 and val_rmse:  0.3824586272239685/n\n",
      "epoch 57 train_mae:  0.02234666422009468 and val_mae:  0.4368647336959839  train_rmse:  0.023288177326321602 and val_rmse:  0.38564392924308777/n\n",
      "epoch 58 train_mae:  0.022576147690415382 and val_mae:  0.3752606511116028  train_rmse:  0.0214731153100729 and val_rmse:  0.3150724470615387/n\n",
      "epoch 59 train_mae:  0.02098914422094822 and val_mae:  0.4161209464073181  train_rmse:  0.019471896812319756 and val_rmse:  0.38823631405830383/n\n",
      "epoch 60 train_mae:  0.021013475954532623 and val_mae:  0.4554835855960846  train_rmse:  0.01996675692498684 and val_rmse:  0.4492073953151703/n\n",
      "epoch 61 train_mae:  0.022471629083156586 and val_mae:  0.4491533041000366  train_rmse:  0.023739367723464966 and val_rmse:  0.39769503474235535/n\n",
      "epoch 62 train_mae:  0.02304060198366642 and val_mae:  0.4476643204689026  train_rmse:  0.022115547209978104 and val_rmse:  0.44424498081207275/n\n",
      "epoch 63 train_mae:  0.024667713791131973 and val_mae:  0.4899728000164032  train_rmse:  0.027322180569171906 and val_rmse:  0.4722735285758972/n\n",
      "epoch 64 train_mae:  0.022280044853687286 and val_mae:  0.4067266285419464  train_rmse:  0.023916594684123993 and val_rmse:  0.36783742904663086/n\n",
      "epoch 65 train_mae:  0.021453777328133583 and val_mae:  0.47026821970939636  train_rmse:  0.019924139603972435 and val_rmse:  0.47104814648628235/n\n",
      "epoch 66 train_mae:  0.026040438562631607 and val_mae:  0.43096789717674255  train_rmse:  0.02755577117204666 and val_rmse:  0.39466267824172974/n\n",
      "epoch 67 train_mae:  0.021316180005669594 and val_mae:  0.5448015332221985  train_rmse:  0.019500890746712685 and val_rmse:  0.5114694833755493/n\n",
      "epoch 68 train_mae:  0.03350915387272835 and val_mae:  0.4882580041885376  train_rmse:  0.04138873890042305 and val_rmse:  0.4499026834964752/n\n",
      "epoch 69 train_mae:  0.026686742901802063 and val_mae:  0.3546883761882782  train_rmse:  0.028036974370479584 and val_rmse:  0.289677232503891/n\n",
      "epoch 70 train_mae:  0.022085249423980713 and val_mae:  0.43016940355300903  train_rmse:  0.02271900326013565 and val_rmse:  0.39826714992523193/n\n",
      "epoch 71 train_mae:  0.02308603748679161 and val_mae:  0.49807336926460266  train_rmse:  0.021974140778183937 and val_rmse:  0.4604145586490631/n\n",
      "epoch 72 train_mae:  0.0235384963452816 and val_mae:  0.3957221806049347  train_rmse:  0.02498438209295273 and val_rmse:  0.3461885452270508/n\n",
      "epoch 73 train_mae:  0.020383555442094803 and val_mae:  0.36725538969039917  train_rmse:  0.01714526303112507 and val_rmse:  0.32127660512924194/n\n",
      "epoch 74 train_mae:  0.022434290498495102 and val_mae:  0.4022217094898224  train_rmse:  0.021070696413517 and val_rmse:  0.36127910017967224/n\n",
      "epoch 75 train_mae:  0.024926936253905296 and val_mae:  0.3523751497268677  train_rmse:  0.024874234572052956 and val_rmse:  0.30041295289993286/n\n",
      "epoch 76 train_mae:  0.021850459277629852 and val_mae:  0.4750041365623474  train_rmse:  0.019008919596672058 and val_rmse:  0.44472041726112366/n\n",
      "epoch 77 train_mae:  0.024780042469501495 and val_mae:  0.3940058648586273  train_rmse:  0.025820519775152206 and val_rmse:  0.3454383313655853/n\n",
      "epoch 78 train_mae:  0.021129721775650978 and val_mae:  0.42270147800445557  train_rmse:  0.019569585099816322 and val_rmse:  0.34254592657089233/n\n",
      "epoch 79 train_mae:  0.02403782308101654 and val_mae:  0.4345862567424774  train_rmse:  0.024588126689195633 and val_rmse:  0.42545270919799805/n\n",
      "epoch 80 train_mae:  0.023791298270225525 and val_mae:  0.3449852168560028  train_rmse:  0.022253945469856262 and val_rmse:  0.27894625067710876/n\n",
      "epoch 81 train_mae:  0.024231432005763054 and val_mae:  0.5504485964775085  train_rmse:  0.028848912566900253 and val_rmse:  0.5264884829521179/n\n",
      "epoch 82 train_mae:  0.02440463937819004 and val_mae:  0.3880106210708618  train_rmse:  0.024904994294047356 and val_rmse:  0.34861916303634644/n\n",
      "epoch 83 train_mae:  0.021366393193602562 and val_mae:  0.4155743718147278  train_rmse:  0.019061848521232605 and val_rmse:  0.3866519629955292/n\n",
      "epoch 84 train_mae:  0.02052932418882847 and val_mae:  0.43368154764175415  train_rmse:  0.017872288823127747 and val_rmse:  0.3765760064125061/n\n",
      "epoch 85 train_mae:  0.02217109687626362 and val_mae:  0.41852301359176636  train_rmse:  0.0214168019592762 and val_rmse:  0.38378384709358215/n\n",
      "epoch 86 train_mae:  0.03584922477602959 and val_mae:  0.5166126489639282  train_rmse:  0.0463426448404789 and val_rmse:  0.49208471179008484/n\n",
      "epoch 87 train_mae:  0.022146129980683327 and val_mae:  0.39449185132980347  train_rmse:  0.02027479186654091 and val_rmse:  0.35274040699005127/n\n",
      "epoch 88 train_mae:  0.02046625129878521 and val_mae:  0.4345409572124481  train_rmse:  0.018617594614624977 and val_rmse:  0.40852129459381104/n\n",
      "epoch 89 train_mae:  0.023511607199907303 and val_mae:  0.3967949450016022  train_rmse:  0.022058086469769478 and val_rmse:  0.3488222658634186/n\n",
      "epoch 90 train_mae:  0.020649421960115433 and val_mae:  0.3559548854827881  train_rmse:  0.019275113940238953 and val_rmse:  0.30754560232162476/n\n",
      "epoch 91 train_mae:  0.023217566311359406 and val_mae:  0.45860105752944946  train_rmse:  0.025918789207935333 and val_rmse:  0.39619195461273193/n\n",
      "epoch 92 train_mae:  0.023318225517868996 and val_mae:  0.43335020542144775  train_rmse:  0.02327166497707367 and val_rmse:  0.39176198840141296/n\n",
      "epoch 93 train_mae:  0.021188754588365555 and val_mae:  0.40849533677101135  train_rmse:  0.018294084817171097 and val_rmse:  0.3439978063106537/n\n",
      "epoch 94 train_mae:  0.02330944687128067 and val_mae:  0.44043245911598206  train_rmse:  0.024139387533068657 and val_rmse:  0.39063841104507446/n\n",
      "epoch 95 train_mae:  0.030195219442248344 and val_mae:  0.39728301763534546  train_rmse:  0.0369039811193943 and val_rmse:  0.3590935170650482/n\n",
      "epoch 96 train_mae:  0.019098227843642235 and val_mae:  0.3415359854698181  train_rmse:  0.01666877046227455 and val_rmse:  0.293213427066803/n\n",
      "epoch 97 train_mae:  0.021710604429244995 and val_mae:  0.3613259792327881  train_rmse:  0.021973030641674995 and val_rmse:  0.31187576055526733/n\n",
      "epoch 98 train_mae:  0.01933462917804718 and val_mae:  0.3548726439476013  train_rmse:  0.01792680285871029 and val_rmse:  0.2952725291252136/n\n",
      "epoch 99 train_mae:  0.02491717040538788 and val_mae:  0.48421692848205566  train_rmse:  0.029760392382740974 and val_rmse:  0.45001694560050964/n\n",
      "epoch 100 train_mae:  0.020262807607650757 and val_mae:  0.37026268243789673  train_rmse:  0.01860438846051693 and val_rmse:  0.3310512602329254/n\n",
      "epoch 101 train_mae:  0.02619442529976368 and val_mae:  0.34679073095321655  train_rmse:  0.03231728449463844 and val_rmse:  0.29337435960769653/n\n",
      "epoch 102 train_mae:  0.02097923494875431 and val_mae:  0.34845462441444397  train_rmse:  0.021018756553530693 and val_rmse:  0.29496222734451294/n\n",
      "epoch 103 train_mae:  0.02022540010511875 and val_mae:  0.3316522240638733  train_rmse:  0.01858680695295334 and val_rmse:  0.28587502241134644/n\n",
      "epoch 104 train_mae:  0.02539193443953991 and val_mae:  0.33852294087409973  train_rmse:  0.025354910641908646 and val_rmse:  0.3024308681488037/n\n",
      "epoch 105 train_mae:  0.02095012366771698 and val_mae:  0.3833988904953003  train_rmse:  0.01856307126581669 and val_rmse:  0.3470715880393982/n\n",
      "epoch 106 train_mae:  0.022487925365567207 and val_mae:  0.3386717736721039  train_rmse:  0.02221178449690342 and val_rmse:  0.29173901677131653/n\n",
      "epoch 107 train_mae:  0.021686721593141556 and val_mae:  0.4247299134731293  train_rmse:  0.021355444565415382 and val_rmse:  0.37184959650039673/n\n",
      "epoch 108 train_mae:  0.018462469801306725 and val_mae:  0.38028594851493835  train_rmse:  0.014983495697379112 and val_rmse:  0.34400880336761475/n\n",
      "epoch 109 train_mae:  0.024505214765667915 and val_mae:  0.34509164094924927  train_rmse:  0.022284895181655884 and val_rmse:  0.2851598262786865/n\n",
      "epoch 110 train_mae:  0.02235395461320877 and val_mae:  0.38007426261901855  train_rmse:  0.022669462487101555 and val_rmse:  0.3218189775943756/n\n",
      "epoch 111 train_mae:  0.02676699496805668 and val_mae:  0.39742347598075867  train_rmse:  0.027866806834936142 and val_rmse:  0.3335607945919037/n\n",
      "epoch 112 train_mae:  0.02295268140733242 and val_mae:  0.4055095911026001  train_rmse:  0.022768059745430946 and val_rmse:  0.3395836353302002/n\n",
      "epoch 113 train_mae:  0.019020555540919304 and val_mae:  0.36522409319877625  train_rmse:  0.016023429110646248 and val_rmse:  0.3065752387046814/n\n",
      "epoch 114 train_mae:  0.02288353443145752 and val_mae:  0.35826295614242554  train_rmse:  0.021852094680070877 and val_rmse:  0.29833051562309265/n\n",
      "epoch 115 train_mae:  0.018337437883019447 and val_mae:  0.4530743956565857  train_rmse:  0.01633726991713047 and val_rmse:  0.40231311321258545/n\n",
      "epoch 116 train_mae:  0.02343382127583027 and val_mae:  0.4516838788986206  train_rmse:  0.024030743166804314 and val_rmse:  0.4106069803237915/n\n",
      "epoch 117 train_mae:  0.021946797147393227 and val_mae:  0.36678826808929443  train_rmse:  0.01954576000571251 and val_rmse:  0.3071257174015045/n\n",
      "epoch 118 train_mae:  0.017471682280302048 and val_mae:  0.3817814886569977  train_rmse:  0.012497690506279469 and val_rmse:  0.3329705595970154/n\n",
      "epoch 119 train_mae:  0.023573940619826317 and val_mae:  0.4357130229473114  train_rmse:  0.02105327881872654 and val_rmse:  0.3942587971687317/n\n",
      "epoch 120 train_mae:  0.022013869136571884 and val_mae:  0.383242130279541  train_rmse:  0.022289970889687538 and val_rmse:  0.34198692440986633/n\n",
      "epoch 121 train_mae:  0.02323571778833866 and val_mae:  0.3774300217628479  train_rmse:  0.025014735758304596 and val_rmse:  0.32512253522872925/n\n",
      "epoch 122 train_mae:  0.021654635667800903 and val_mae:  0.34227731823921204  train_rmse:  0.01995619758963585 and val_rmse:  0.2858198285102844/n\n",
      "epoch 123 train_mae:  0.027609528973698616 and val_mae:  0.3955358862876892  train_rmse:  0.028065549209713936 and val_rmse:  0.3352995812892914/n\n",
      "epoch 124 train_mae:  0.019993538036942482 and val_mae:  0.37939780950546265  train_rmse:  0.01683085225522518 and val_rmse:  0.32273781299591064/n\n",
      "epoch 125 train_mae:  0.01865251362323761 and val_mae:  0.35348597168922424  train_rmse:  0.015661902725696564 and val_rmse:  0.2954130172729492/n\n",
      "epoch 126 train_mae:  0.020554035902023315 and val_mae:  0.40812948346138  train_rmse:  0.016544466838240623 and val_rmse:  0.3585655987262726/n\n",
      "epoch 127 train_mae:  0.021415051072835922 and val_mae:  0.362923264503479  train_rmse:  0.020687725394964218 and val_rmse:  0.30128222703933716/n\n",
      "epoch 128 train_mae:  0.01806098222732544 and val_mae:  0.34761637449264526  train_rmse:  0.015052841044962406 and val_rmse:  0.28633958101272583/n\n",
      "epoch 129 train_mae:  0.02268219366669655 and val_mae:  0.38946565985679626  train_rmse:  0.0213138647377491 and val_rmse:  0.3255627453327179/n\n",
      "epoch 130 train_mae:  0.01857675611972809 and val_mae:  0.40459662675857544  train_rmse:  0.014559543691575527 and val_rmse:  0.34909480810165405/n\n",
      "epoch 131 train_mae:  0.02111453376710415 and val_mae:  0.36529144644737244  train_rmse:  0.019090045243501663 and val_rmse:  0.30558082461357117/n\n",
      "epoch 132 train_mae:  0.022574011236429214 and val_mae:  0.34896332025527954  train_rmse:  0.020678473636507988 and val_rmse:  0.29356980323791504/n\n",
      "epoch 133 train_mae:  0.019918855279684067 and val_mae:  0.3448425233364105  train_rmse:  0.01795567385852337 and val_rmse:  0.2854648530483246/n\n",
      "epoch 134 train_mae:  0.023007897660136223 and val_mae:  0.3635680377483368  train_rmse:  0.02193990908563137 and val_rmse:  0.2985592782497406/n\n",
      "epoch 135 train_mae:  0.022216040641069412 and val_mae:  0.3538074791431427  train_rmse:  0.021096548065543175 and val_rmse:  0.29209810495376587/n\n",
      "epoch 136 train_mae:  0.02279314398765564 and val_mae:  0.370477557182312  train_rmse:  0.021025970578193665 and val_rmse:  0.30994758009910583/n\n",
      "epoch 137 train_mae:  0.02093522623181343 and val_mae:  0.3794262111186981  train_rmse:  0.019563695415854454 and val_rmse:  0.3216937780380249/n\n",
      "epoch 138 train_mae:  0.024740038439631462 and val_mae:  0.37285399436950684  train_rmse:  0.026177698746323586 and val_rmse:  0.3222503960132599/n\n",
      "epoch 139 train_mae:  0.020489899441599846 and val_mae:  0.3670458495616913  train_rmse:  0.01789005473256111 and val_rmse:  0.3149929940700531/n\n",
      "epoch 140 train_mae:  0.022243423387408257 and val_mae:  0.4079846739768982  train_rmse:  0.021118557080626488 and val_rmse:  0.34818851947784424/n\n",
      "epoch 141 train_mae:  0.020760543644428253 and val_mae:  0.377795547246933  train_rmse:  0.01658722199499607 and val_rmse:  0.316826730966568/n\n",
      "epoch 142 train_mae:  0.01918814517557621 and val_mae:  0.3402033746242523  train_rmse:  0.014928238466382027 and val_rmse:  0.2793293297290802/n\n",
      "epoch 143 train_mae:  0.02765960432589054 and val_mae:  0.3613983988761902  train_rmse:  0.03232269361615181 and val_rmse:  0.29353246092796326/n\n",
      "epoch 144 train_mae:  0.020727558061480522 and val_mae:  0.3476793169975281  train_rmse:  0.01831166446208954 and val_rmse:  0.2861059308052063/n\n",
      "epoch 145 train_mae:  0.024527646601200104 and val_mae:  0.37954217195510864  train_rmse:  0.022648291662335396 and val_rmse:  0.3212887942790985/n\n",
      "epoch 146 train_mae:  0.021375099197030067 and val_mae:  0.3965148329734802  train_rmse:  0.021190987899899483 and val_rmse:  0.3422834873199463/n\n",
      "epoch 147 train_mae:  0.019449463114142418 and val_mae:  0.3439996540546417  train_rmse:  0.015209730714559555 and val_rmse:  0.28215670585632324/n\n",
      "epoch 148 train_mae:  0.0204473864287138 and val_mae:  0.3490317463874817  train_rmse:  0.017730027437210083 and val_rmse:  0.28595831990242004/n\n",
      "epoch 149 train_mae:  0.022847019135951996 and val_mae:  0.3499881327152252  train_rmse:  0.01995186321437359 and val_rmse:  0.2897055745124817/n\n",
      "epoch 150 train_mae:  0.021921230480074883 and val_mae:  0.37796127796173096  train_rmse:  0.021018946543335915 and val_rmse:  0.32159996032714844/n\n",
      "epoch 151 train_mae:  0.018339168280363083 and val_mae:  0.34609824419021606  train_rmse:  0.014963076449930668 and val_rmse:  0.2831348478794098/n\n",
      "epoch 152 train_mae:  0.02599921077489853 and val_mae:  0.3257465362548828  train_rmse:  0.02621353231370449 and val_rmse:  0.2670225501060486/n\n",
      "epoch 153 train_mae:  0.023838672786951065 and val_mae:  0.36341148614883423  train_rmse:  0.023174477741122246 and val_rmse:  0.31007322669029236/n\n",
      "epoch 154 train_mae:  0.02138677053153515 and val_mae:  0.3906925916671753  train_rmse:  0.019653884693980217 and val_rmse:  0.32928115129470825/n\n",
      "epoch 155 train_mae:  0.023979563266038895 and val_mae:  0.3462413251399994  train_rmse:  0.024260129779577255 and val_rmse:  0.28673678636550903/n\n",
      "epoch 156 train_mae:  0.021355515345931053 and val_mae:  0.3602758049964905  train_rmse:  0.018789710476994514 and val_rmse:  0.3040446639060974/n\n",
      "epoch 157 train_mae:  0.021081605926156044 and val_mae:  0.38204169273376465  train_rmse:  0.0198358166962862 and val_rmse:  0.32290881872177124/n\n",
      "epoch 158 train_mae:  0.021872013807296753 and val_mae:  0.34279686212539673  train_rmse:  0.020798224955797195 and val_rmse:  0.27810990810394287/n\n",
      "epoch 159 train_mae:  0.025630619376897812 and val_mae:  0.4184303283691406  train_rmse:  0.03154543787240982 and val_rmse:  0.3545067012310028/n\n",
      "epoch 160 train_mae:  0.02172325737774372 and val_mae:  0.38664281368255615  train_rmse:  0.022189663723111153 and val_rmse:  0.32668277621269226/n\n",
      "epoch 161 train_mae:  0.023962346836924553 and val_mae:  0.40530925989151  train_rmse:  0.025055980309844017 and val_rmse:  0.34850165247917175/n\n",
      "epoch 162 train_mae:  0.01650680974125862 and val_mae:  0.338158518075943  train_rmse:  0.012063651345670223 and val_rmse:  0.27869170904159546/n\n",
      "epoch 163 train_mae:  0.022431202232837677 and val_mae:  0.3630542755126953  train_rmse:  0.02092030644416809 and val_rmse:  0.3072247803211212/n\n",
      "epoch 164 train_mae:  0.02483432926237583 and val_mae:  0.371222585439682  train_rmse:  0.027423007413744926 and val_rmse:  0.31552642583847046/n\n",
      "epoch 165 train_mae:  0.023903630673885345 and val_mae:  0.4230004549026489  train_rmse:  0.0277818962931633 and val_rmse:  0.370443731546402/n\n",
      "epoch 166 train_mae:  0.021571414545178413 and val_mae:  0.38109123706817627  train_rmse:  0.020588982850313187 and val_rmse:  0.3249509334564209/n\n",
      "epoch 167 train_mae:  0.01923278532922268 and val_mae:  0.38810470700263977  train_rmse:  0.015724018216133118 and val_rmse:  0.3311117887496948/n\n",
      "epoch 168 train_mae:  0.022801969200372696 and val_mae:  0.37286266684532166  train_rmse:  0.022249681875109673 and val_rmse:  0.3172345757484436/n\n",
      "epoch 169 train_mae:  0.018726464360952377 and val_mae:  0.37391397356987  train_rmse:  0.015883110463619232 and val_rmse:  0.316364049911499/n\n",
      "epoch 170 train_mae:  0.021016109734773636 and val_mae:  0.3465976119041443  train_rmse:  0.022270893678069115 and val_rmse:  0.2922106683254242/n\n",
      "epoch 171 train_mae:  0.02303292602300644 and val_mae:  0.35382184386253357  train_rmse:  0.023285701870918274 and val_rmse:  0.2937515377998352/n\n",
      "epoch 172 train_mae:  0.01831178180873394 and val_mae:  0.3520667254924774  train_rmse:  0.01440892368555069 and val_rmse:  0.2961166203022003/n\n",
      "epoch 173 train_mae:  0.031791478395462036 and val_mae:  0.3909223675727844  train_rmse:  0.03585052490234375 and val_rmse:  0.3334426283836365/n\n",
      "epoch 174 train_mae:  0.025852717459201813 and val_mae:  0.3543887734413147  train_rmse:  0.02637624368071556 and val_rmse:  0.30124399065971375/n\n",
      "epoch 175 train_mae:  0.020771313458681107 and val_mae:  0.34950122237205505  train_rmse:  0.018366500735282898 and val_rmse:  0.28997746109962463/n\n",
      "epoch 176 train_mae:  0.02020970918238163 and val_mae:  0.33848321437835693  train_rmse:  0.01901054009795189 and val_rmse:  0.287835955619812/n\n",
      "epoch 177 train_mae:  0.024919377639889717 and val_mae:  0.36085665225982666  train_rmse:  0.024678179994225502 and val_rmse:  0.3083932399749756/n\n",
      "epoch 178 train_mae:  0.021637704223394394 and val_mae:  0.37584009766578674  train_rmse:  0.02116626873612404 and val_rmse:  0.3199412524700165/n\n",
      "epoch 179 train_mae:  0.022617368027567863 and val_mae:  0.3510391414165497  train_rmse:  0.022330392152071 and val_rmse:  0.29483088850975037/n\n",
      "epoch 180 train_mae:  0.022233111783862114 and val_mae:  0.3483334183692932  train_rmse:  0.019418207928538322 and val_rmse:  0.2908470332622528/n\n",
      "epoch 181 train_mae:  0.022358691319823265 and val_mae:  0.3801997900009155  train_rmse:  0.023149916902184486 and val_rmse:  0.3229701519012451/n\n",
      "epoch 182 train_mae:  0.019693391397595406 and val_mae:  0.3736186623573303  train_rmse:  0.01627865433692932 and val_rmse:  0.3206372857093811/n\n",
      "epoch 183 train_mae:  0.021012451499700546 and val_mae:  0.3741930425167084  train_rmse:  0.019936854019761086 and val_rmse:  0.3209525942802429/n\n",
      "epoch 184 train_mae:  0.018047204241156578 and val_mae:  0.3619319498538971  train_rmse:  0.014766344800591469 and val_rmse:  0.3075367212295532/n\n",
      "epoch 185 train_mae:  0.020144512876868248 and val_mae:  0.3740684986114502  train_rmse:  0.020692750811576843 and val_rmse:  0.3171887695789337/n\n",
      "epoch 186 train_mae:  0.02238844521343708 and val_mae:  0.3447548449039459  train_rmse:  0.024042969569563866 and val_rmse:  0.2894914150238037/n\n",
      "epoch 187 train_mae:  0.020398633554577827 and val_mae:  0.3452395498752594  train_rmse:  0.016972703859210014 and val_rmse:  0.28885403275489807/n\n",
      "epoch 188 train_mae:  0.02016625367105007 and val_mae:  0.3432384431362152  train_rmse:  0.01736169494688511 and val_rmse:  0.28535401821136475/n\n",
      "epoch 189 train_mae:  0.024502871558070183 and val_mae:  0.3695022761821747  train_rmse:  0.02536631003022194 and val_rmse:  0.3117170035839081/n\n",
      "epoch 190 train_mae:  0.018617749214172363 and val_mae:  0.3760194480419159  train_rmse:  0.016669105738401413 and val_rmse:  0.31446439027786255/n\n",
      "epoch 191 train_mae:  0.018358265981078148 and val_mae:  0.3606633245944977  train_rmse:  0.013502064161002636 and val_rmse:  0.30399003624916077/n\n",
      "epoch 192 train_mae:  0.020733460783958435 and val_mae:  0.3533630669116974  train_rmse:  0.019574429839849472 and val_rmse:  0.29690781235694885/n\n",
      "epoch 193 train_mae:  0.01772523857653141 and val_mae:  0.3756164312362671  train_rmse:  0.015740299597382545 and val_rmse:  0.3199673295021057/n\n",
      "epoch 194 train_mae:  0.017541198059916496 and val_mae:  0.3604842722415924  train_rmse:  0.013883141800761223 and val_rmse:  0.3058811128139496/n\n",
      "epoch 195 train_mae:  0.024932080879807472 and val_mae:  0.3597671389579773  train_rmse:  0.02722994051873684 and val_rmse:  0.30512484908103943/n\n",
      "epoch 196 train_mae:  0.01893782801926136 and val_mae:  0.34225550293922424  train_rmse:  0.01502680592238903 and val_rmse:  0.28625041246414185/n\n",
      "epoch 197 train_mae:  0.0231841579079628 and val_mae:  0.4032599627971649  train_rmse:  0.02132447063922882 and val_rmse:  0.3467645049095154/n\n",
      "epoch 198 train_mae:  0.021859019994735718 and val_mae:  0.37198373675346375  train_rmse:  0.018939267843961716 and val_rmse:  0.31369316577911377/n\n",
      "epoch 199 train_mae:  0.02041642740368843 and val_mae:  0.3900619149208069  train_rmse:  0.016589244827628136 and val_rmse:  0.33162549138069153/n\n",
      "epoch 200 train_mae:  0.022949278354644775 and val_mae:  0.38698720932006836  train_rmse:  0.019583512097597122 and val_rmse:  0.332134485244751/n\n",
      "epoch 201 train_mae:  0.021242866292595863 and val_mae:  0.39005571603775024  train_rmse:  0.018062256276607513 and val_rmse:  0.33266681432724/n\n",
      "epoch 202 train_mae:  0.020747918635606766 and val_mae:  0.4081025719642639  train_rmse:  0.017077285796403885 and val_rmse:  0.3543088138103485/n\n",
      "epoch 203 train_mae:  0.02085789479315281 and val_mae:  0.3513992428779602  train_rmse:  0.02018097974359989 and val_rmse:  0.29324275255203247/n\n",
      "epoch 204 train_mae:  0.025056855753064156 and val_mae:  0.3490021824836731  train_rmse:  0.022936690598726273 and val_rmse:  0.29230332374572754/n\n",
      "epoch 205 train_mae:  0.0198013074696064 and val_mae:  0.38721904158592224  train_rmse:  0.016249880194664 and val_rmse:  0.328786700963974/n\n",
      "epoch 206 train_mae:  0.021622247993946075 and val_mae:  0.356180340051651  train_rmse:  0.019631264731287956 and val_rmse:  0.30133241415023804/n\n",
      "epoch 207 train_mae:  0.020156558603048325 and val_mae:  0.3584437966346741  train_rmse:  0.01753549836575985 and val_rmse:  0.30317994952201843/n\n",
      "epoch 208 train_mae:  0.01906508393585682 and val_mae:  0.3719298839569092  train_rmse:  0.01708914339542389 and val_rmse:  0.31800228357315063/n\n",
      "epoch 209 train_mae:  0.019931841641664505 and val_mae:  0.3650420308113098  train_rmse:  0.01865520514547825 and val_rmse:  0.3080753684043884/n\n",
      "epoch 210 train_mae:  0.02010120265185833 and val_mae:  0.3838373124599457  train_rmse:  0.018179727718234062 and val_rmse:  0.33175283670425415/n\n",
      "epoch 211 train_mae:  0.020316829904913902 and val_mae:  0.3814989924430847  train_rmse:  0.01949782855808735 and val_rmse:  0.33010271191596985/n\n",
      "epoch 212 train_mae:  0.02099183015525341 and val_mae:  0.36679187417030334  train_rmse:  0.019617944955825806 and val_rmse:  0.30698323249816895/n\n",
      "epoch 213 train_mae:  0.020529475063085556 and val_mae:  0.38753604888916016  train_rmse:  0.01687893271446228 and val_rmse:  0.33061233162879944/n\n",
      "epoch 214 train_mae:  0.021764472126960754 and val_mae:  0.34130075573921204  train_rmse:  0.021397534757852554 and val_rmse:  0.28369638323783875/n\n",
      "epoch 215 train_mae:  0.022403733804821968 and val_mae:  0.3450380563735962  train_rmse:  0.024643447250127792 and val_rmse:  0.28979411721229553/n\n",
      "epoch 216 train_mae:  0.019007686525583267 and val_mae:  0.3457985520362854  train_rmse:  0.014346503652632236 and val_rmse:  0.2890608310699463/n\n",
      "epoch 217 train_mae:  0.021788503974676132 and val_mae:  0.35944291949272156  train_rmse:  0.024763742461800575 and val_rmse:  0.3047827184200287/n\n",
      "epoch 218 train_mae:  0.020029950886964798 and val_mae:  0.3623896539211273  train_rmse:  0.018122075125575066 and val_rmse:  0.3091011047363281/n\n",
      "epoch 219 train_mae:  0.021241620182991028 and val_mae:  0.38938313722610474  train_rmse:  0.020557796582579613 and val_rmse:  0.3331261873245239/n\n",
      "epoch 220 train_mae:  0.019953835755586624 and val_mae:  0.4095624089241028  train_rmse:  0.02400374785065651 and val_rmse:  0.3544195592403412/n\n",
      "epoch 221 train_mae:  0.02160196378827095 and val_mae:  0.3851381838321686  train_rmse:  0.020459184423089027 and val_rmse:  0.33115294575691223/n\n",
      "epoch 222 train_mae:  0.022531207650899887 and val_mae:  0.35342562198638916  train_rmse:  0.02154555916786194 and val_rmse:  0.3048144578933716/n\n",
      "epoch 223 train_mae:  0.02423618920147419 and val_mae:  0.3836175799369812  train_rmse:  0.02799256518483162 and val_rmse:  0.3295940160751343/n\n",
      "epoch 224 train_mae:  0.020321059972047806 and val_mae:  0.36088523268699646  train_rmse:  0.0194744523614645 and val_rmse:  0.303113728761673/n\n",
      "epoch 225 train_mae:  0.019639188423752785 and val_mae:  0.3711702823638916  train_rmse:  0.01757911965250969 and val_rmse:  0.31816044449806213/n\n",
      "epoch 226 train_mae:  0.018665194511413574 and val_mae:  0.3270021975040436  train_rmse:  0.015758071094751358 and val_rmse:  0.26886653900146484/n\n",
      "epoch 227 train_mae:  0.021356163546442986 and val_mae:  0.3568113148212433  train_rmse:  0.0223039910197258 and val_rmse:  0.3019762337207794/n\n",
      "epoch 228 train_mae:  0.021046046167612076 and val_mae:  0.3565136790275574  train_rmse:  0.023699939250946045 and val_rmse:  0.30167752504348755/n\n",
      "epoch 229 train_mae:  0.021627260372042656 and val_mae:  0.3491303622722626  train_rmse:  0.021811310201883316 and val_rmse:  0.29547104239463806/n\n",
      "epoch 230 train_mae:  0.020289435982704163 and val_mae:  0.3408381938934326  train_rmse:  0.016799647361040115 and val_rmse:  0.2854590117931366/n\n",
      "epoch 231 train_mae:  0.019877320155501366 and val_mae:  0.3453977704048157  train_rmse:  0.015789290890097618 and val_rmse:  0.28710833191871643/n\n",
      "epoch 232 train_mae:  0.020635372027754784 and val_mae:  0.3583837151527405  train_rmse:  0.020472893491387367 and val_rmse:  0.3021826446056366/n\n",
      "epoch 233 train_mae:  0.01831328310072422 and val_mae:  0.36249902844429016  train_rmse:  0.017519254237413406 and val_rmse:  0.30660176277160645/n\n",
      "epoch 234 train_mae:  0.0244259275496006 and val_mae:  0.3737383484840393  train_rmse:  0.025431903079152107 and val_rmse:  0.3169823884963989/n\n",
      "epoch 235 train_mae:  0.020918486639857292 and val_mae:  0.3605004549026489  train_rmse:  0.020385362207889557 and val_rmse:  0.3079950511455536/n\n",
      "epoch 236 train_mae:  0.02000468038022518 and val_mae:  0.35267025232315063  train_rmse:  0.01600077748298645 and val_rmse:  0.3002641201019287/n\n",
      "epoch 237 train_mae:  0.02211850881576538 and val_mae:  0.3422004282474518  train_rmse:  0.0219400767236948 and val_rmse:  0.2857270836830139/n\n",
      "epoch 238 train_mae:  0.02363610826432705 and val_mae:  0.36270320415496826  train_rmse:  0.024265561252832413 and val_rmse:  0.31014713644981384/n\n",
      "epoch 239 train_mae:  0.022355543449521065 and val_mae:  0.36963343620300293  train_rmse:  0.02122577652335167 and val_rmse:  0.3111022412776947/n\n",
      "epoch 240 train_mae:  0.02547689527273178 and val_mae:  0.36516013741493225  train_rmse:  0.030502168461680412 and val_rmse:  0.3134639263153076/n\n",
      "epoch 241 train_mae:  0.020324604585766792 and val_mae:  0.3340719938278198  train_rmse:  0.018997756764292717 and val_rmse:  0.2806689739227295/n\n",
      "epoch 242 train_mae:  0.02190089039504528 and val_mae:  0.3592824935913086  train_rmse:  0.021178651601076126 and val_rmse:  0.3049786686897278/n\n",
      "epoch 243 train_mae:  0.02130981720983982 and val_mae:  0.37016427516937256  train_rmse:  0.018399344757199287 and val_rmse:  0.31650397181510925/n\n",
      "epoch 244 train_mae:  0.02315814234316349 and val_mae:  0.355843186378479  train_rmse:  0.021320249885320663 and val_rmse:  0.30533140897750854/n\n",
      "epoch 245 train_mae:  0.021038567647337914 and val_mae:  0.36109623312950134  train_rmse:  0.020982563495635986 and val_rmse:  0.30578914284706116/n\n",
      "epoch 246 train_mae:  0.02427014335989952 and val_mae:  0.3731098771095276  train_rmse:  0.022977812215685844 and val_rmse:  0.31855759024620056/n\n",
      "epoch 247 train_mae:  0.019617890939116478 and val_mae:  0.34158360958099365  train_rmse:  0.01789831928908825 and val_rmse:  0.28648653626441956/n\n",
      "epoch 248 train_mae:  0.022927850484848022 and val_mae:  0.3718760907649994  train_rmse:  0.0223386213183403 and val_rmse:  0.31980401277542114/n\n",
      "epoch 249 train_mae:  0.020071761682629585 and val_mae:  0.36194196343421936  train_rmse:  0.016498377546668053 and val_rmse:  0.30239737033843994/n\n",
      "epoch 250 train_mae:  0.026615366339683533 and val_mae:  0.3293563723564148  train_rmse:  0.028328130021691322 and val_rmse:  0.27434104681015015/n\n",
      "epoch 251 train_mae:  0.0183476060628891 and val_mae:  0.33821627497673035  train_rmse:  0.014402154833078384 and val_rmse:  0.2821904122829437/n\n",
      "epoch 252 train_mae:  0.02178092673420906 and val_mae:  0.3855064809322357  train_rmse:  0.019471019506454468 and val_rmse:  0.32701587677001953/n\n",
      "epoch 253 train_mae:  0.02559923566877842 and val_mae:  0.3505028486251831  train_rmse:  0.0249944981187582 and val_rmse:  0.2941625118255615/n\n",
      "epoch 254 train_mae:  0.021656304597854614 and val_mae:  0.3487265706062317  train_rmse:  0.020781120285391808 and val_rmse:  0.2986339330673218/n\n",
      "epoch 255 train_mae:  0.021835390478372574 and val_mae:  0.3425425589084625  train_rmse:  0.020358284935355186 and val_rmse:  0.2908496558666229/n\n",
      "epoch 256 train_mae:  0.024291882291436195 and val_mae:  0.3673328459262848  train_rmse:  0.023830771446228027 and val_rmse:  0.31252554059028625/n\n",
      "epoch 257 train_mae:  0.016562912613153458 and val_mae:  0.3534737229347229  train_rmse:  0.013034151867032051 and val_rmse:  0.29895976185798645/n\n",
      "epoch 258 train_mae:  0.022752026095986366 and val_mae:  0.3727593719959259  train_rmse:  0.023441795259714127 and val_rmse:  0.31881648302078247/n\n",
      "epoch 259 train_mae:  0.02192508615553379 and val_mae:  0.39501506090164185  train_rmse:  0.02232253924012184 and val_rmse:  0.3385143280029297/n\n",
      "epoch 260 train_mae:  0.021867139264941216 and val_mae:  0.3500059247016907  train_rmse:  0.02094866894185543 and val_rmse:  0.29556211829185486/n\n",
      "epoch 261 train_mae:  0.020642057061195374 and val_mae:  0.35543128848075867  train_rmse:  0.017233945429325104 and val_rmse:  0.30189186334609985/n\n",
      "epoch 262 train_mae:  0.023106610402464867 and val_mae:  0.3595936894416809  train_rmse:  0.023367423564195633 and val_rmse:  0.3026706576347351/n\n",
      "epoch 263 train_mae:  0.01861082762479782 and val_mae:  0.3623676300048828  train_rmse:  0.01751936413347721 and val_rmse:  0.3091588616371155/n\n",
      "epoch 264 train_mae:  0.02152874879539013 and val_mae:  0.38605064153671265  train_rmse:  0.023691100999712944 and val_rmse:  0.3333317041397095/n\n",
      "epoch 265 train_mae:  0.020645523443818092 and val_mae:  0.3484483063220978  train_rmse:  0.017846308648586273 and val_rmse:  0.2934260666370392/n\n",
      "epoch 266 train_mae:  0.02229810692369938 and val_mae:  0.3725815713405609  train_rmse:  0.021039310842752457 and val_rmse:  0.3151521384716034/n\n",
      "epoch 267 train_mae:  0.024756619706749916 and val_mae:  0.3882187008857727  train_rmse:  0.023152386769652367 and val_rmse:  0.33442261815071106/n\n",
      "epoch 268 train_mae:  0.018581651151180267 and val_mae:  0.3784440755844116  train_rmse:  0.014456051401793957 and val_rmse:  0.32653680443763733/n\n",
      "epoch 269 train_mae:  0.023241091519594193 and val_mae:  0.3919675052165985  train_rmse:  0.023685498163104057 and val_rmse:  0.33409687876701355/n\n",
      "epoch 270 train_mae:  0.020732739940285683 and val_mae:  0.38032034039497375  train_rmse:  0.01946231722831726 and val_rmse:  0.32805895805358887/n\n",
      "epoch 271 train_mae:  0.02418450452387333 and val_mae:  0.36421889066696167  train_rmse:  0.024313244968652725 and val_rmse:  0.3074732720851898/n\n",
      "epoch 272 train_mae:  0.022429514676332474 and val_mae:  0.37765005230903625  train_rmse:  0.020754477009177208 and val_rmse:  0.3199186325073242/n\n",
      "epoch 273 train_mae:  0.021595848724246025 and val_mae:  0.4064984917640686  train_rmse:  0.021785080432891846 and val_rmse:  0.34834226965904236/n\n",
      "epoch 274 train_mae:  0.023249998688697815 and val_mae:  0.35172945261001587  train_rmse:  0.02140544354915619 and val_rmse:  0.29739901423454285/n\n",
      "epoch 275 train_mae:  0.020662588998675346 and val_mae:  0.3430405557155609  train_rmse:  0.020702997222542763 and val_rmse:  0.28645166754722595/n\n",
      "epoch 276 train_mae:  0.022330090403556824 and val_mae:  0.36520516872406006  train_rmse:  0.01958037167787552 and val_rmse:  0.3070856034755707/n\n",
      "epoch 277 train_mae:  0.020128674805164337 and val_mae:  0.36564838886260986  train_rmse:  0.01759377121925354 and val_rmse:  0.31242308020591736/n\n",
      "epoch 278 train_mae:  0.01975492388010025 and val_mae:  0.3796970844268799  train_rmse:  0.01766381226480007 and val_rmse:  0.3259516954421997/n\n",
      "epoch 279 train_mae:  0.016997408121824265 and val_mae:  0.38230040669441223  train_rmse:  0.012021607719361782 and val_rmse:  0.32699811458587646/n\n",
      "epoch 280 train_mae:  0.021212173625826836 and val_mae:  0.36317405104637146  train_rmse:  0.02045331709086895 and val_rmse:  0.3108364939689636/n\n",
      "epoch 281 train_mae:  0.020699163898825645 and val_mae:  0.34894800186157227  train_rmse:  0.021397871896624565 and val_rmse:  0.29545003175735474/n\n",
      "epoch 282 train_mae:  0.023929912596940994 and val_mae:  0.3395242393016815  train_rmse:  0.025273887440562248 and val_rmse:  0.2815439999103546/n\n",
      "epoch 283 train_mae:  0.02314012125134468 and val_mae:  0.3848600685596466  train_rmse:  0.021605774760246277 and val_rmse:  0.32971909642219543/n\n",
      "epoch 284 train_mae:  0.02411934733390808 and val_mae:  0.3436698019504547  train_rmse:  0.02582576312124729 and val_rmse:  0.28776565194129944/n\n",
      "epoch 285 train_mae:  0.022417737171053886 and val_mae:  0.33835387229919434  train_rmse:  0.021006613969802856 and val_rmse:  0.28146907687187195/n\n",
      "epoch 286 train_mae:  0.019587043672800064 and val_mae:  0.3703431487083435  train_rmse:  0.016458634287118912 and val_rmse:  0.3134057819843292/n\n",
      "epoch 287 train_mae:  0.02034594491124153 and val_mae:  0.3730292022228241  train_rmse:  0.018682362511754036 and val_rmse:  0.31908994913101196/n\n",
      "epoch 288 train_mae:  0.03120526857674122 and val_mae:  0.3793678879737854  train_rmse:  0.036193326115608215 and val_rmse:  0.32555216550827026/n\n",
      "epoch 289 train_mae:  0.021643005311489105 and val_mae:  0.37182483077049255  train_rmse:  0.020254995673894882 and val_rmse:  0.3149948716163635/n\n",
      "epoch 290 train_mae:  0.021011216565966606 and val_mae:  0.3630140721797943  train_rmse:  0.018473569303750992 and val_rmse:  0.3080909848213196/n\n",
      "epoch 291 train_mae:  0.020395616069436073 and val_mae:  0.3645692765712738  train_rmse:  0.0184126365929842 and val_rmse:  0.31133154034614563/n\n",
      "epoch 292 train_mae:  0.023361332714557648 and val_mae:  0.35659343004226685  train_rmse:  0.022387243807315826 and val_rmse:  0.2995849549770355/n\n",
      "epoch 293 train_mae:  0.018920646980404854 and val_mae:  0.35790351033210754  train_rmse:  0.015759408473968506 and val_rmse:  0.30041784048080444/n\n",
      "epoch 294 train_mae:  0.020109189674258232 and val_mae:  0.3604682683944702  train_rmse:  0.01735011860728264 and val_rmse:  0.30504703521728516/n\n",
      "epoch 295 train_mae:  0.024561749771237373 and val_mae:  0.3919909596443176  train_rmse:  0.023743288591504097 and val_rmse:  0.33822470903396606/n\n",
      "epoch 296 train_mae:  0.019034890457987785 and val_mae:  0.3745744526386261  train_rmse:  0.016789089888334274 and val_rmse:  0.31605854630470276/n\n",
      "epoch 297 train_mae:  0.01953962817788124 and val_mae:  0.36684468388557434  train_rmse:  0.016825472936034203 and val_rmse:  0.3106767237186432/n\n",
      "epoch 298 train_mae:  0.028334055095911026 and val_mae:  0.34929999709129333  train_rmse:  0.03405537083745003 and val_rmse:  0.29348936676979065/n\n",
      "epoch 299 train_mae:  0.022051770240068436 and val_mae:  0.38101041316986084  train_rmse:  0.023179609328508377 and val_rmse:  0.3253624439239502/n\n",
      "epoch 300 train_mae:  0.02282821014523506 and val_mae:  0.34969982504844666  train_rmse:  0.020407630130648613 and val_rmse:  0.2917095720767975/n\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, loss, optimizer, n_epochs, device, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15d032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbede72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e9c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc8c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9a277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785c3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c058d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698537a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a164cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2032a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028c95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418140e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516957e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43056d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b43d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ce693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cd47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3553103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
