{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371 177\n"
     ]
    }
   ],
   "source": [
    "#导入模型参数\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"not removing hydrogen atom without neighbors\")\n",
    "import sys\n",
    "sys.path.append(\"/home/dwj/WWW/DDIsubgraph/pretrain\")\n",
    "from dataset.dataset import MoleculeDatasetWrapper\n",
    "from models.model import Model001\n",
    "from loss_utils.nt_xent import NTXentLoss\n",
    "from loss_utils.weighted_nt_xent import Weighted_NTXentLoss\n",
    "from loss_utils.motif_loss import Motif_Loss\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=1,\n",
    "                    help='which gpu to use if any (default: 0)')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help=' inputbatch size for training (default: 1024)')\n",
    "parser.add_argument('--dataset', type=str, default='/home/dwj/WWW/DDIsubgraph/pretrain/data/Compsol.csv',\n",
    "                    help='root directory of dataset.')\n",
    "\n",
    "parser.add_argument('--valid_size', type=float, default=0.05,\n",
    "                    help='valid_size (default: 0.2)')\n",
    "parser.add_argument('--num_workers', type=int, default=0,\n",
    "                    help=' the number of workers to load data (default: 8)')\n",
    "\n",
    "\n",
    "parser.add_argument('--num_layer', type=int, default=4,\n",
    "                    help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                    help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--feat_dim', type=int, default=300,\n",
    "                    help='embedding dimensions (default: 256)')\n",
    "parser.add_argument('--dropout_gin', type=float, default=0,\n",
    "                    help='dropout ratio (default: 0.2)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                    help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--N', type=int, default=2,\n",
    "                    help='num layer of transformer encoder')\n",
    "parser.add_argument('--d_model', type=int, default=256,\n",
    "                    help='embedding dimensions (default: 256)')\n",
    "parser.add_argument('--d_ff', type=int, default=1024,\n",
    "                    help='embedding dimensions (default: 1024)')\n",
    "parser.add_argument('--h', type=int, default=8,\n",
    "                    help='heads of transformer encoder(default: 8)')\n",
    "parser.add_argument('--dropout_encoder', type=float, default=0.1,\n",
    "                    help='dropout ratio (default: 0.1)')\n",
    "parser.add_argument('--weight', type=int, default=1,\n",
    "                    help='weight or not')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "\n",
    "parser.add_argument('--decay', type=float, default=0.00001,\n",
    "                    help='weight decay (default: 0)')\n",
    "parser.add_argument('--lr', type=float, default=0.0005,\n",
    "                    help='learning rate (default: 0.001)')\n",
    "\n",
    "\n",
    "parser.add_argument('--output_model_file', type=str, default='/home/dwj/WWW/DDIsubgraph/pretrain/save_model/pretrain/motif_loss+weight',\n",
    "                    help='filename to output the pre-trained model')\n",
    "parser.add_argument('--gama', type=float, default=0.2, help='weight of motif_loss')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "dataset = MoleculeDatasetWrapper(args.batch_size, args.num_workers, args.valid_size, args.dataset)   # dataset：一个txt文件路径，里面村的是smiles\n",
    "train_loader, valid_loader = dataset.get_data_loaders()  # 导入数据集，编码，在dataset里\n",
    "model = Model001(num_layer=args.num_layer, emb_dim=args.emb_dim, feat_dim=args.feat_dim,\n",
    "                 dropout_gin=args.dropout_gin, pool=args.graph_pooling, device=device,\n",
    "                 N=args.N, d_model=args.d_model, d_ff=args.d_ff, h=args.h,\n",
    "                 dropout_encoder=args.dropout_encoder).to(device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a40b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f71e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "import layers1\n",
    "import models1\n",
    "import custom_loss\n",
    "import time\n",
    "import torch.nn as nn\n",
    "#'C','N','O', 'S','F','Si','P', 'Cl','Br','Mg','Na','Ca','Fe','As','Al','I','B','V','K','Tl',\n",
    "#            'Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn','H', 'Li','Ge','Cu','Au','Ni','Cd','In',\n",
    "#            'Mn','Zr','Cr','Pt','Hg','Pb','Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f80ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_atom_feats = 55\n",
    "n_atom_hid = 256\n",
    "rel_total = 86\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 300\n",
    "neg_samples = 1\n",
    "batch_size = 1024\n",
    "data_size_ratio = 1\n",
    "kge_dim = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbcd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(pred, target):\n",
    "    labels_tensor = torch.tensor([float(x) for x in target], dtype=torch.float32).unsqueeze(1).to(pred.device)\n",
    "    return F.mse_loss(pred, labels_tensor )\n",
    "def mae_loss(pred, target):\n",
    "    # 将目标值转换为浮点数张量，并调整形状以匹配预测值\n",
    "    labels_tensor = torch.tensor([float(x) for x in target], dtype=torch.float32).unsqueeze(1).to(pred.device)\n",
    "    # 计算绝对误差\n",
    "    absolute_errors = torch.abs(pred - labels_tensor)\n",
    "    # 返回平均绝对误差\n",
    "    return torch.mean(absolute_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737fa54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, val_data_loader, loss_fn,  optimizer, n_epochs, device, scheduler=None):\n",
    "    print('Starting training at', datetime.today())\n",
    "    for i in range(1, n_epochs+1):\n",
    "        start = time.time()\n",
    "        train_loss = 0\n",
    "        train_loss_pos = 0\n",
    "        train_loss_neg = 0\n",
    "        val_loss = 0\n",
    "        val_loss_pos = 0\n",
    "        val_loss_neg = 0\n",
    "\n",
    "        sum1=0\n",
    "        for batch in train_data_loader:\n",
    "            model.train()\n",
    "            p_score = model(batch,device)\n",
    "            loss= rmse_loss(p_score,batch[2])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum1+=len(batch[2]) \n",
    "            train_mae = mae_loss(p_score,batch[2])\n",
    "            train_mae += train_mae.item() * len(p_score)\n",
    "            train_rmse = rmse_loss(p_score,batch[2])\n",
    "            train_rmse += train_rmse.item() * len(p_score)\n",
    "        train_mae /= sum1\n",
    "        train_rmse /= sum1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sum2=0\n",
    "            for batch in val_data_loader:\n",
    "                model.eval()\n",
    "                p_score = model(batch,device)\n",
    "                loss= rmse_loss(p_score,batch[2]) \n",
    "                #print(len(batch[2])  ) \n",
    "                 \n",
    "                sum2+=len(batch[2])\n",
    "                val_mae = mae_loss(p_score,batch[2])       \n",
    "                val_mae += val_mae.item() * len(p_score)\n",
    "                val_rmse = rmse_loss(p_score,batch[2])       \n",
    "                val_rmse += val_rmse.item() * len(p_score)\n",
    "            val_mae /= sum2\n",
    "            val_rmse /= sum2\n",
    "               \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"epoch {i} train_mae:  {train_mae} and val_mae:  {val_mae}  train_rmse:  {train_rmse} and val_rmse:  {val_rmse}/n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ff474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = custom_loss.SigmoidLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.96 ** (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d803ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2024-09-05 19:44:16.289914\n",
      "epoch 1 train_mae:  0.05292055010795593 and val_mae:  1.7791236639022827  train_rmse:  0.1321474015712738 and val_rmse:  4.883437156677246/n\n",
      "epoch 2 train_mae:  0.04661503806710243 and val_mae:  1.6452324390411377  train_rmse:  0.10689503699541092 and val_rmse:  4.876831531524658/n\n",
      "epoch 3 train_mae:  0.057056035846471786 and val_mae:  1.82615327835083  train_rmse:  0.1343628615140915 and val_rmse:  4.957463264465332/n\n",
      "epoch 4 train_mae:  0.05591604858636856 and val_mae:  1.6994951963424683  train_rmse:  0.13764172792434692 and val_rmse:  4.718388557434082/n\n",
      "epoch 5 train_mae:  0.046449124813079834 and val_mae:  1.6967103481292725  train_rmse:  0.11426348239183426 and val_rmse:  4.670897006988525/n\n",
      "epoch 6 train_mae:  0.05461429804563522 and val_mae:  1.7735737562179565  train_rmse:  0.12629222869873047 and val_rmse:  4.775106906890869/n\n",
      "epoch 7 train_mae:  0.04656542092561722 and val_mae:  1.751615285873413  train_rmse:  0.10241900384426117 and val_rmse:  4.715847015380859/n\n",
      "epoch 8 train_mae:  0.06311323493719101 and val_mae:  1.7777457237243652  train_rmse:  0.1566048115491867 and val_rmse:  4.731365203857422/n\n",
      "epoch 9 train_mae:  0.047949086874723434 and val_mae:  1.693193793296814  train_rmse:  0.10845725983381271 and val_rmse:  4.389798641204834/n\n",
      "epoch 10 train_mae:  0.05410982668399811 and val_mae:  1.7023128271102905  train_rmse:  0.1323327273130417 and val_rmse:  4.348670959472656/n\n",
      "epoch 11 train_mae:  0.03935706242918968 and val_mae:  1.1522252559661865  train_rmse:  0.07184381783008575 and val_rmse:  2.2841317653656006/n\n",
      "epoch 12 train_mae:  0.032525453716516495 and val_mae:  1.2145612239837646  train_rmse:  0.04467673599720001 and val_rmse:  2.5088589191436768/n\n",
      "epoch 13 train_mae:  0.03806018456816673 and val_mae:  1.0811058282852173  train_rmse:  0.0676359310746193 and val_rmse:  2.294820785522461/n\n",
      "epoch 14 train_mae:  0.03978604078292847 and val_mae:  1.009847640991211  train_rmse:  0.06487040221691132 and val_rmse:  2.024244785308838/n\n",
      "epoch 15 train_mae:  0.03870503231883049 and val_mae:  0.9750600457191467  train_rmse:  0.062086112797260284 and val_rmse:  1.8630170822143555/n\n",
      "epoch 16 train_mae:  0.03539838269352913 and val_mae:  0.9594371318817139  train_rmse:  0.061742011457681656 and val_rmse:  1.9591060876846313/n\n",
      "epoch 17 train_mae:  0.03011133149266243 and val_mae:  0.9073176383972168  train_rmse:  0.04565340280532837 and val_rmse:  1.700627326965332/n\n",
      "epoch 18 train_mae:  0.03153073787689209 and val_mae:  0.9229312539100647  train_rmse:  0.050637174397706985 and val_rmse:  1.7918281555175781/n\n",
      "epoch 19 train_mae:  0.03046388179063797 and val_mae:  0.9150770902633667  train_rmse:  0.047833774238824844 and val_rmse:  1.703107237815857/n\n",
      "epoch 20 train_mae:  0.03067450039088726 and val_mae:  0.8794583082199097  train_rmse:  0.048290129750967026 and val_rmse:  1.6495639085769653/n\n",
      "epoch 21 train_mae:  0.03098411113023758 and val_mae:  0.877922534942627  train_rmse:  0.05649057775735855 and val_rmse:  1.6150633096694946/n\n",
      "epoch 22 train_mae:  0.02820291742682457 and val_mae:  0.7674369812011719  train_rmse:  0.036473702639341354 and val_rmse:  1.3397772312164307/n\n",
      "epoch 23 train_mae:  0.026259329169988632 and val_mae:  0.8413276672363281  train_rmse:  0.0367233045399189 and val_rmse:  1.5134660005569458/n\n",
      "epoch 24 train_mae:  0.035571347922086716 and val_mae:  0.8670260906219482  train_rmse:  0.078973688185215 and val_rmse:  1.5800610780715942/n\n",
      "epoch 25 train_mae:  0.025700781494379044 and val_mae:  0.7388937473297119  train_rmse:  0.03607868775725365 and val_rmse:  1.228726863861084/n\n",
      "epoch 26 train_mae:  0.024532293900847435 and val_mae:  0.765630304813385  train_rmse:  0.031099382787942886 and val_rmse:  1.4322305917739868/n\n",
      "epoch 27 train_mae:  0.03651080280542374 and val_mae:  0.7935991287231445  train_rmse:  0.07408418506383896 and val_rmse:  1.3699626922607422/n\n",
      "epoch 28 train_mae:  0.024049675092101097 and val_mae:  0.889662504196167  train_rmse:  0.027159223333001137 and val_rmse:  1.605130672454834/n\n",
      "epoch 29 train_mae:  0.02268797531723976 and val_mae:  0.6861699819564819  train_rmse:  0.023679444566369057 and val_rmse:  1.0139272212982178/n\n",
      "epoch 30 train_mae:  0.027463767677545547 and val_mae:  0.6919685006141663  train_rmse:  0.03328234329819679 and val_rmse:  0.9631606936454773/n\n",
      "epoch 31 train_mae:  0.023252014070749283 and val_mae:  0.6777536869049072  train_rmse:  0.030627725645899773 and val_rmse:  0.9284627437591553/n\n",
      "epoch 32 train_mae:  0.026890750974416733 and val_mae:  0.7380730509757996  train_rmse:  0.040632348507642746 and val_rmse:  1.109215497970581/n\n",
      "epoch 33 train_mae:  0.02316918969154358 and val_mae:  0.6698441505432129  train_rmse:  0.023488743230700493 and val_rmse:  0.9459533095359802/n\n",
      "epoch 34 train_mae:  0.024154815822839737 and val_mae:  0.6894853115081787  train_rmse:  0.027010736986994743 and val_rmse:  0.9299455285072327/n\n",
      "epoch 35 train_mae:  0.023233093321323395 and val_mae:  0.7208287119865417  train_rmse:  0.026219988241791725 and val_rmse:  1.0234593152999878/n\n",
      "epoch 36 train_mae:  0.029482033103704453 and val_mae:  0.722688615322113  train_rmse:  0.034264616668224335 and val_rmse:  0.9347667098045349/n\n",
      "epoch 37 train_mae:  0.022628862410783768 and val_mae:  0.6695824265480042  train_rmse:  0.020288217812776566 and val_rmse:  0.8663541078567505/n\n",
      "epoch 38 train_mae:  0.023318026214838028 and val_mae:  0.6463087201118469  train_rmse:  0.03290882706642151 and val_rmse:  0.8345448970794678/n\n",
      "epoch 39 train_mae:  0.021234476938843727 and val_mae:  0.6483368873596191  train_rmse:  0.018319446593523026 and val_rmse:  0.8497862815856934/n\n",
      "epoch 40 train_mae:  0.02184889279305935 and val_mae:  0.6211116909980774  train_rmse:  0.02733699418604374 and val_rmse:  0.7605937719345093/n\n",
      "epoch 41 train_mae:  0.021692484617233276 and val_mae:  0.6165124177932739  train_rmse:  0.019386960193514824 and val_rmse:  0.7763991951942444/n\n",
      "epoch 42 train_mae:  0.023015443235635757 and val_mae:  0.6614811420440674  train_rmse:  0.03058069385588169 and val_rmse:  0.8418360948562622/n\n",
      "epoch 43 train_mae:  0.02356780134141445 and val_mae:  0.7095795273780823  train_rmse:  0.022789642214775085 and val_rmse:  0.8839195370674133/n\n",
      "epoch 44 train_mae:  0.021379632875323296 and val_mae:  0.6466786861419678  train_rmse:  0.02295912615954876 and val_rmse:  0.8691748380661011/n\n",
      "epoch 45 train_mae:  0.018470894545316696 and val_mae:  0.6156337857246399  train_rmse:  0.014983804896473885 and val_rmse:  0.746332585811615/n\n",
      "epoch 46 train_mae:  0.019147396087646484 and val_mae:  0.6781424283981323  train_rmse:  0.01877214014530182 and val_rmse:  0.8614900708198547/n\n",
      "epoch 47 train_mae:  0.02118176780641079 and val_mae:  0.5868147015571594  train_rmse:  0.03164505586028099 and val_rmse:  0.697725236415863/n\n",
      "epoch 48 train_mae:  0.01976783759891987 and val_mae:  0.6021209359169006  train_rmse:  0.022457391023635864 and val_rmse:  0.7115009427070618/n\n",
      "epoch 49 train_mae:  0.02067338488996029 and val_mae:  0.6526579856872559  train_rmse:  0.018762841820716858 and val_rmse:  0.7438269257545471/n\n",
      "epoch 50 train_mae:  0.022198263555765152 and val_mae:  0.6688336730003357  train_rmse:  0.024390527978539467 and val_rmse:  0.9320160150527954/n\n",
      "epoch 51 train_mae:  0.01917240582406521 and val_mae:  0.5905486941337585  train_rmse:  0.01798122189939022 and val_rmse:  0.7250099182128906/n\n",
      "epoch 52 train_mae:  0.018131187185645103 and val_mae:  0.5756454467773438  train_rmse:  0.014657411724328995 and val_rmse:  0.6982941031455994/n\n",
      "epoch 53 train_mae:  0.01923312060534954 and val_mae:  0.5704712867736816  train_rmse:  0.022211352363228798 and val_rmse:  0.6611765623092651/n\n",
      "epoch 54 train_mae:  0.01729445531964302 and val_mae:  0.5588125586509705  train_rmse:  0.012688972055912018 and val_rmse:  0.6659733057022095/n\n",
      "epoch 55 train_mae:  0.02070525847375393 and val_mae:  0.5551758408546448  train_rmse:  0.022677544504404068 and val_rmse:  0.6287916302680969/n\n",
      "epoch 56 train_mae:  0.0167364664375782 and val_mae:  0.5822177529335022  train_rmse:  0.012653605081140995 and val_rmse:  0.6975746154785156/n\n",
      "epoch 57 train_mae:  0.019651740789413452 and val_mae:  0.5511559844017029  train_rmse:  0.01965232379734516 and val_rmse:  0.604966938495636/n\n",
      "epoch 58 train_mae:  0.01841500774025917 and val_mae:  0.5534238815307617  train_rmse:  0.021095439791679382 and val_rmse:  0.6157982349395752/n\n",
      "epoch 59 train_mae:  0.021778933703899384 and val_mae:  0.5383707284927368  train_rmse:  0.02884429320693016 and val_rmse:  0.5854364037513733/n\n",
      "epoch 60 train_mae:  0.015854723751544952 and val_mae:  0.5316195487976074  train_rmse:  0.015262452885508537 and val_rmse:  0.5615652799606323/n\n",
      "epoch 61 train_mae:  0.017965558916330338 and val_mae:  0.5353139042854309  train_rmse:  0.016595089808106422 and val_rmse:  0.5875847935676575/n\n",
      "epoch 62 train_mae:  0.017407024279236794 and val_mae:  0.5352903604507446  train_rmse:  0.017518380656838417 and val_rmse:  0.5797775387763977/n\n",
      "epoch 63 train_mae:  0.015854066237807274 and val_mae:  0.5238521099090576  train_rmse:  0.011481452733278275 and val_rmse:  0.5958102345466614/n\n",
      "epoch 64 train_mae:  0.017591871321201324 and val_mae:  0.5309160351753235  train_rmse:  0.017343906685709953 and val_rmse:  0.5589703321456909/n\n",
      "epoch 65 train_mae:  0.017191795632243156 and val_mae:  0.5249543786048889  train_rmse:  0.01713557541370392 and val_rmse:  0.5469681620597839/n\n",
      "epoch 66 train_mae:  0.017166484147310257 and val_mae:  0.5361731052398682  train_rmse:  0.015465077944099903 and val_rmse:  0.6167950630187988/n\n",
      "epoch 67 train_mae:  0.017528334632515907 and val_mae:  0.5148724913597107  train_rmse:  0.013116391375660896 and val_rmse:  0.5327458381652832/n\n",
      "epoch 68 train_mae:  0.016112923622131348 and val_mae:  0.5380577445030212  train_rmse:  0.012236865237355232 and val_rmse:  0.6180694103240967/n\n",
      "epoch 69 train_mae:  0.017902066931128502 and val_mae:  0.5214875936508179  train_rmse:  0.017373232170939445 and val_rmse:  0.5370203852653503/n\n",
      "epoch 70 train_mae:  0.0139137739315629 and val_mae:  0.4939453899860382  train_rmse:  0.008797087706625462 and val_rmse:  0.5117438435554504/n\n",
      "epoch 71 train_mae:  0.015615098178386688 and val_mae:  0.49969926476478577  train_rmse:  0.013065336272120476 and val_rmse:  0.5222231149673462/n\n",
      "epoch 72 train_mae:  0.017196130007505417 and val_mae:  0.47918376326560974  train_rmse:  0.018770096823573112 and val_rmse:  0.47754693031311035/n\n",
      "epoch 73 train_mae:  0.013537449762225151 and val_mae:  0.4733804166316986  train_rmse:  0.007512248586863279 and val_rmse:  0.47537827491760254/n\n",
      "epoch 74 train_mae:  0.015069960616528988 and val_mae:  0.49106094241142273  train_rmse:  0.010906962677836418 and val_rmse:  0.5229363441467285/n\n",
      "epoch 75 train_mae:  0.016137847676873207 and val_mae:  0.4863327443599701  train_rmse:  0.0162817370146513 and val_rmse:  0.5302268862724304/n\n",
      "epoch 76 train_mae:  0.018155574798583984 and val_mae:  0.49018600583076477  train_rmse:  0.01819489151239395 and val_rmse:  0.4953004717826843/n\n",
      "epoch 77 train_mae:  0.015982557088136673 and val_mae:  0.4741430878639221  train_rmse:  0.015030978247523308 and val_rmse:  0.46941477060317993/n\n",
      "epoch 78 train_mae:  0.017679473385214806 and val_mae:  0.479160338640213  train_rmse:  0.017217176035046577 and val_rmse:  0.4705110788345337/n\n",
      "epoch 79 train_mae:  0.018581107258796692 and val_mae:  0.4746254086494446  train_rmse:  0.019095808267593384 and val_rmse:  0.5137624740600586/n\n",
      "epoch 80 train_mae:  0.013962479308247566 and val_mae:  0.4740147888660431  train_rmse:  0.00906115211546421 and val_rmse:  0.47878941893577576/n\n",
      "epoch 81 train_mae:  0.017356693744659424 and val_mae:  0.4817378520965576  train_rmse:  0.025135463103652 and val_rmse:  0.4870861768722534/n\n",
      "epoch 82 train_mae:  0.014892907813191414 and val_mae:  0.4682871699333191  train_rmse:  0.010200915858149529 and val_rmse:  0.45813989639282227/n\n",
      "epoch 83 train_mae:  0.014853057451546192 and val_mae:  0.48097872734069824  train_rmse:  0.012370922602713108 and val_rmse:  0.4816795289516449/n\n",
      "epoch 84 train_mae:  0.013638726435601711 and val_mae:  0.47333791851997375  train_rmse:  0.009387325495481491 and val_rmse:  0.46665120124816895/n\n",
      "epoch 85 train_mae:  0.014660933054983616 and val_mae:  0.48151925206184387  train_rmse:  0.010526214726269245 and val_rmse:  0.4869247376918793/n\n",
      "epoch 86 train_mae:  0.01746608503162861 and val_mae:  0.4767163395881653  train_rmse:  0.018094491213560104 and val_rmse:  0.4838986098766327/n\n",
      "epoch 87 train_mae:  0.013124357908964157 and val_mae:  0.45981091260910034  train_rmse:  0.007524180691689253 and val_rmse:  0.4563696086406708/n\n",
      "epoch 88 train_mae:  0.014773497357964516 and val_mae:  0.47117581963539124  train_rmse:  0.010837418958544731 and val_rmse:  0.470490425825119/n\n",
      "epoch 89 train_mae:  0.014521526172757149 and val_mae:  0.46825024485588074  train_rmse:  0.009111099876463413 and val_rmse:  0.47923409938812256/n\n",
      "epoch 90 train_mae:  0.014524748548865318 and val_mae:  0.46259036660194397  train_rmse:  0.011721963062882423 and val_rmse:  0.45868822932243347/n\n",
      "epoch 91 train_mae:  0.016342222690582275 and val_mae:  0.4631149172782898  train_rmse:  0.01964484341442585 and val_rmse:  0.46550238132476807/n\n",
      "epoch 92 train_mae:  0.012826282531023026 and val_mae:  0.4601428508758545  train_rmse:  0.008534596301615238 and val_rmse:  0.4569628834724426/n\n",
      "epoch 93 train_mae:  0.013768061064183712 and val_mae:  0.46300840377807617  train_rmse:  0.009876635856926441 and val_rmse:  0.4647599756717682/n\n",
      "epoch 94 train_mae:  0.013540903106331825 and val_mae:  0.45542728900909424  train_rmse:  0.009017380885779858 and val_rmse:  0.4513280987739563/n\n",
      "epoch 95 train_mae:  0.013771582394838333 and val_mae:  0.46116161346435547  train_rmse:  0.01018237229436636 and val_rmse:  0.44621366262435913/n\n",
      "epoch 96 train_mae:  0.014321777038276196 and val_mae:  0.45540204644203186  train_rmse:  0.014157013036310673 and val_rmse:  0.45263203978538513/n\n",
      "epoch 97 train_mae:  0.014761177822947502 and val_mae:  0.4541682302951813  train_rmse:  0.014349729754030704 and val_rmse:  0.4484546482563019/n\n",
      "epoch 98 train_mae:  0.014666454866528511 and val_mae:  0.4594188928604126  train_rmse:  0.012193778529763222 and val_rmse:  0.4522395431995392/n\n",
      "epoch 99 train_mae:  0.015824556350708008 and val_mae:  0.4579503536224365  train_rmse:  0.012085938826203346 and val_rmse:  0.4626273214817047/n\n",
      "epoch 100 train_mae:  0.015300881117582321 and val_mae:  0.4519712030887604  train_rmse:  0.011049660854041576 and val_rmse:  0.4407714605331421/n\n",
      "epoch 101 train_mae:  0.016246501356363297 and val_mae:  0.45615148544311523  train_rmse:  0.018068967387080193 and val_rmse:  0.43543216586112976/n\n",
      "epoch 102 train_mae:  0.017060622572898865 and val_mae:  0.4638749957084656  train_rmse:  0.015015856362879276 and val_rmse:  0.44566819071769714/n\n",
      "epoch 103 train_mae:  0.016787609085440636 and val_mae:  0.4538211524486542  train_rmse:  0.013078548945486546 and val_rmse:  0.4490331709384918/n\n",
      "epoch 104 train_mae:  0.01836463250219822 and val_mae:  0.4522051215171814  train_rmse:  0.01877126842737198 and val_rmse:  0.44448620080947876/n\n",
      "epoch 105 train_mae:  0.014971061609685421 and val_mae:  0.45809000730514526  train_rmse:  0.011585095897316933 and val_rmse:  0.44408097863197327/n\n",
      "epoch 106 train_mae:  0.013367083854973316 and val_mae:  0.4565258324146271  train_rmse:  0.007850823923945427 and val_rmse:  0.44695451855659485/n\n",
      "epoch 107 train_mae:  0.013681148178875446 and val_mae:  0.4455084502696991  train_rmse:  0.008715069852769375 and val_rmse:  0.43077561259269714/n\n",
      "epoch 108 train_mae:  0.012402606196701527 and val_mae:  0.45328548550605774  train_rmse:  0.007579520810395479 and val_rmse:  0.4384433925151825/n\n",
      "epoch 109 train_mae:  0.012908450327813625 and val_mae:  0.4463014602661133  train_rmse:  0.007488363888114691 and val_rmse:  0.43739575147628784/n\n",
      "epoch 110 train_mae:  0.012969456613063812 and val_mae:  0.4480040371417999  train_rmse:  0.008289686404168606 and val_rmse:  0.4285847246646881/n\n",
      "epoch 111 train_mae:  0.013307539746165276 and val_mae:  0.4597185254096985  train_rmse:  0.00804371852427721 and val_rmse:  0.4468748867511749/n\n",
      "epoch 112 train_mae:  0.013322866521775723 and val_mae:  0.4526113271713257  train_rmse:  0.008562957867980003 and val_rmse:  0.4334855079650879/n\n",
      "epoch 113 train_mae:  0.01345202699303627 and val_mae:  0.4510330557823181  train_rmse:  0.009030108340084553 and val_rmse:  0.4187532067298889/n\n",
      "epoch 114 train_mae:  0.011493357829749584 and val_mae:  0.4525827467441559  train_rmse:  0.005237787961959839 and val_rmse:  0.4320971965789795/n\n",
      "epoch 115 train_mae:  0.015051955357193947 and val_mae:  0.44861292839050293  train_rmse:  0.013298033736646175 and val_rmse:  0.44638457894325256/n\n",
      "epoch 116 train_mae:  0.012922244146466255 and val_mae:  0.4484615921974182  train_rmse:  0.008489882573485374 and val_rmse:  0.43039238452911377/n\n",
      "epoch 117 train_mae:  0.013508920557796955 and val_mae:  0.4480222463607788  train_rmse:  0.008678874000906944 and val_rmse:  0.4248461127281189/n\n",
      "epoch 118 train_mae:  0.011365196667611599 and val_mae:  0.45074978470802307  train_rmse:  0.006243820767849684 and val_rmse:  0.42859458923339844/n\n",
      "epoch 119 train_mae:  0.013661227189004421 and val_mae:  0.4465525150299072  train_rmse:  0.010991659015417099 and val_rmse:  0.4293574392795563/n\n",
      "epoch 120 train_mae:  0.013804876245558262 and val_mae:  0.4467674493789673  train_rmse:  0.01081492006778717 and val_rmse:  0.43648484349250793/n\n",
      "epoch 121 train_mae:  0.012183223851025105 and val_mae:  0.4444549083709717  train_rmse:  0.010062291286885738 and val_rmse:  0.4208265542984009/n\n",
      "epoch 122 train_mae:  0.013821669854223728 and val_mae:  0.4499826431274414  train_rmse:  0.008648942224681377 and val_rmse:  0.41967302560806274/n\n",
      "epoch 123 train_mae:  0.011915653944015503 and val_mae:  0.4494672417640686  train_rmse:  0.00703355623409152 and val_rmse:  0.425531268119812/n\n",
      "epoch 124 train_mae:  0.01293493714183569 and val_mae:  0.44572922587394714  train_rmse:  0.007964114658534527 and val_rmse:  0.42924749851226807/n\n",
      "epoch 125 train_mae:  0.01486391481012106 and val_mae:  0.4509894847869873  train_rmse:  0.01235400140285492 and val_rmse:  0.42770788073539734/n\n",
      "epoch 126 train_mae:  0.014454344287514687 and val_mae:  0.44639426469802856  train_rmse:  0.012628721073269844 and val_rmse:  0.4305814206600189/n\n",
      "epoch 127 train_mae:  0.012377251870930195 and val_mae:  0.44771838188171387  train_rmse:  0.008310467004776001 and val_rmse:  0.4236886501312256/n\n",
      "epoch 128 train_mae:  0.012525791302323341 and val_mae:  0.4511117935180664  train_rmse:  0.007651441264897585 and val_rmse:  0.4235202968120575/n\n",
      "epoch 129 train_mae:  0.013270268216729164 and val_mae:  0.4438304305076599  train_rmse:  0.008645261637866497 and val_rmse:  0.42498451471328735/n\n",
      "epoch 130 train_mae:  0.012843353673815727 and val_mae:  0.4475550651550293  train_rmse:  0.007078430149704218 and val_rmse:  0.4282720983028412/n\n",
      "epoch 131 train_mae:  0.013186396099627018 and val_mae:  0.44555604457855225  train_rmse:  0.008188135921955109 and val_rmse:  0.4250578284263611/n\n",
      "epoch 132 train_mae:  0.01532589178532362 and val_mae:  0.44657477736473083  train_rmse:  0.020900525152683258 and val_rmse:  0.42381516098976135/n\n",
      "epoch 133 train_mae:  0.014531506225466728 and val_mae:  0.44604575634002686  train_rmse:  0.011897483840584755 and val_rmse:  0.42663320899009705/n\n",
      "epoch 134 train_mae:  0.014920580200850964 and val_mae:  0.44606298208236694  train_rmse:  0.012870472855865955 and val_rmse:  0.41878941655158997/n\n",
      "epoch 135 train_mae:  0.012528601102530956 and val_mae:  0.4454347789287567  train_rmse:  0.009600283578038216 and val_rmse:  0.4219565987586975/n\n",
      "epoch 136 train_mae:  0.014429428614675999 and val_mae:  0.44426429271698  train_rmse:  0.01149132288992405 and val_rmse:  0.42024046182632446/n\n",
      "epoch 137 train_mae:  0.014688373543322086 and val_mae:  0.4432862102985382  train_rmse:  0.011742159724235535 and val_rmse:  0.41600459814071655/n\n",
      "epoch 138 train_mae:  0.012843860313296318 and val_mae:  0.44646942615509033  train_rmse:  0.008242691867053509 and val_rmse:  0.4223216474056244/n\n",
      "epoch 139 train_mae:  0.012661971151828766 and val_mae:  0.44592922925949097  train_rmse:  0.008020536974072456 and val_rmse:  0.4162091612815857/n\n",
      "epoch 140 train_mae:  0.017054002732038498 and val_mae:  0.4472557604312897  train_rmse:  0.02212342992424965 and val_rmse:  0.42260560393333435/n\n",
      "epoch 141 train_mae:  0.015443417243659496 and val_mae:  0.4453502595424652  train_rmse:  0.019567152485251427 and val_rmse:  0.4216430187225342/n\n",
      "epoch 142 train_mae:  0.014134877361357212 and val_mae:  0.44567281007766724  train_rmse:  0.011525346897542477 and val_rmse:  0.415450781583786/n\n",
      "epoch 143 train_mae:  0.014305704273283482 and val_mae:  0.4450550675392151  train_rmse:  0.010217512957751751 and val_rmse:  0.4203871786594391/n\n",
      "epoch 144 train_mae:  0.014159474521875381 and val_mae:  0.4459208548069  train_rmse:  0.01027714740484953 and val_rmse:  0.4203363358974457/n\n",
      "epoch 145 train_mae:  0.01279324758797884 and val_mae:  0.44810378551483154  train_rmse:  0.008106492459774017 and val_rmse:  0.4175871014595032/n\n",
      "epoch 146 train_mae:  0.014837471768260002 and val_mae:  0.4453151822090149  train_rmse:  0.014047078788280487 and val_rmse:  0.42019861936569214/n\n",
      "epoch 147 train_mae:  0.014773287810385227 and val_mae:  0.44351863861083984  train_rmse:  0.011423047631978989 and val_rmse:  0.41605156660079956/n\n",
      "epoch 148 train_mae:  0.013586883433163166 and val_mae:  0.44480302929878235  train_rmse:  0.008063489571213722 and val_rmse:  0.4180522561073303/n\n",
      "epoch 149 train_mae:  0.013289480470120907 and val_mae:  0.4453137218952179  train_rmse:  0.009465940296649933 and val_rmse:  0.4189082682132721/n\n",
      "epoch 150 train_mae:  0.01181812398135662 and val_mae:  0.44456303119659424  train_rmse:  0.0069423578679561615 and val_rmse:  0.4185371696949005/n\n",
      "epoch 151 train_mae:  0.013144878670573235 and val_mae:  0.4451783001422882  train_rmse:  0.009761880151927471 and val_rmse:  0.42084118723869324/n\n",
      "epoch 152 train_mae:  0.01162540353834629 and val_mae:  0.4452655613422394  train_rmse:  0.006323426030576229 and val_rmse:  0.41774022579193115/n\n",
      "epoch 153 train_mae:  0.01335722953081131 and val_mae:  0.44322314858436584  train_rmse:  0.01011597365140915 and val_rmse:  0.4164978265762329/n\n",
      "epoch 154 train_mae:  0.011237663216888905 and val_mae:  0.44443556666374207  train_rmse:  0.005523624364286661 and val_rmse:  0.4179269075393677/n\n",
      "epoch 155 train_mae:  0.014653565362095833 and val_mae:  0.4446600377559662  train_rmse:  0.011948530562222004 and val_rmse:  0.4177013337612152/n\n",
      "epoch 156 train_mae:  0.01201476901769638 and val_mae:  0.44488057494163513  train_rmse:  0.006344576366245747 and val_rmse:  0.4174346625804901/n\n",
      "epoch 157 train_mae:  0.013095246627926826 and val_mae:  0.44421082735061646  train_rmse:  0.008050894364714622 and val_rmse:  0.4184654951095581/n\n",
      "epoch 158 train_mae:  0.01116086170077324 and val_mae:  0.4444476068019867  train_rmse:  0.006151266396045685 and val_rmse:  0.4176284074783325/n\n",
      "epoch 159 train_mae:  0.013917248696088791 and val_mae:  0.44374510645866394  train_rmse:  0.008966267108917236 and val_rmse:  0.4166885018348694/n\n",
      "epoch 160 train_mae:  0.01322463434189558 and val_mae:  0.44400930404663086  train_rmse:  0.011725053191184998 and val_rmse:  0.4174828827381134/n\n",
      "epoch 161 train_mae:  0.011991016566753387 and val_mae:  0.443205863237381  train_rmse:  0.006562951486557722 and val_rmse:  0.41783684492111206/n\n",
      "epoch 162 train_mae:  0.014429337345063686 and val_mae:  0.44389429688453674  train_rmse:  0.010258727706968784 and val_rmse:  0.41680917143821716/n\n",
      "epoch 163 train_mae:  0.013325157575309277 and val_mae:  0.4438646137714386  train_rmse:  0.00917863380163908 and val_rmse:  0.4194386899471283/n\n",
      "epoch 164 train_mae:  0.015483665280044079 and val_mae:  0.444824755191803  train_rmse:  0.021883981302380562 and val_rmse:  0.41753003001213074/n\n",
      "epoch 165 train_mae:  0.011577554047107697 and val_mae:  0.4442282021045685  train_rmse:  0.005708945915102959 and val_rmse:  0.415748655796051/n\n",
      "epoch 166 train_mae:  0.011003547348082066 and val_mae:  0.444143682718277  train_rmse:  0.0065246038138866425 and val_rmse:  0.4172589182853699/n\n",
      "epoch 167 train_mae:  0.011804761365056038 and val_mae:  0.443366140127182  train_rmse:  0.007948949001729488 and val_rmse:  0.4160143733024597/n\n",
      "epoch 168 train_mae:  0.01269779447466135 and val_mae:  0.4440629184246063  train_rmse:  0.006753724534064531 and val_rmse:  0.41783276200294495/n\n",
      "epoch 169 train_mae:  0.013684497214853764 and val_mae:  0.4438387155532837  train_rmse:  0.00908758956938982 and val_rmse:  0.41585448384284973/n\n",
      "epoch 170 train_mae:  0.015690507367253304 and val_mae:  0.4435226321220398  train_rmse:  0.014643103815615177 and val_rmse:  0.41550537943840027/n\n",
      "epoch 171 train_mae:  0.013113369233906269 and val_mae:  0.4436286687850952  train_rmse:  0.008014531806111336 and val_rmse:  0.4161064624786377/n\n",
      "epoch 172 train_mae:  0.011916971765458584 and val_mae:  0.44499754905700684  train_rmse:  0.006625038105994463 and val_rmse:  0.4175110459327698/n\n",
      "epoch 173 train_mae:  0.011684559285640717 and val_mae:  0.4435322880744934  train_rmse:  0.006866536568850279 and val_rmse:  0.4159173369407654/n\n",
      "epoch 174 train_mae:  0.014627443626523018 and val_mae:  0.443379282951355  train_rmse:  0.011095994152128696 and val_rmse:  0.41780510544776917/n\n",
      "epoch 175 train_mae:  0.0138630960136652 and val_mae:  0.4439524710178375  train_rmse:  0.009179746732115746 and val_rmse:  0.4160482585430145/n\n",
      "epoch 176 train_mae:  0.01666990853846073 and val_mae:  0.44369634985923767  train_rmse:  0.01750452257692814 and val_rmse:  0.4163130819797516/n\n",
      "epoch 177 train_mae:  0.01329852920025587 and val_mae:  0.44305089116096497  train_rmse:  0.00927771907299757 and val_rmse:  0.41612496972084045/n\n",
      "epoch 178 train_mae:  0.011437409557402134 and val_mae:  0.4434387683868408  train_rmse:  0.00649828789755702 and val_rmse:  0.41600626707077026/n\n",
      "epoch 179 train_mae:  0.013961036689579487 and val_mae:  0.4435414969921112  train_rmse:  0.01180450338870287 and val_rmse:  0.4167419373989105/n\n",
      "epoch 180 train_mae:  0.01313179824501276 and val_mae:  0.4434729218482971  train_rmse:  0.01033808197826147 and val_rmse:  0.4166874289512634/n\n",
      "epoch 181 train_mae:  0.013764291070401669 and val_mae:  0.4440130591392517  train_rmse:  0.008988712914288044 and val_rmse:  0.4149719774723053/n\n",
      "epoch 182 train_mae:  0.013106674887239933 and val_mae:  0.44366076588630676  train_rmse:  0.010596230626106262 and val_rmse:  0.4164915084838867/n\n",
      "epoch 183 train_mae:  0.013703723438084126 and val_mae:  0.4427696466445923  train_rmse:  0.010524074546992779 and val_rmse:  0.41673755645751953/n\n",
      "epoch 184 train_mae:  0.0131929200142622 and val_mae:  0.4433048665523529  train_rmse:  0.008350055664777756 and val_rmse:  0.41518262028694153/n\n",
      "epoch 185 train_mae:  0.01312321238219738 and val_mae:  0.44334548711776733  train_rmse:  0.008061950094997883 and val_rmse:  0.41590777039527893/n\n",
      "epoch 186 train_mae:  0.0120028555393219 and val_mae:  0.4436059296131134  train_rmse:  0.007066892925649881 and val_rmse:  0.4162748157978058/n\n",
      "epoch 187 train_mae:  0.012695170938968658 and val_mae:  0.44357579946517944  train_rmse:  0.0072609675116837025 and val_rmse:  0.4158323407173157/n\n",
      "epoch 188 train_mae:  0.014058665372431278 and val_mae:  0.4435994327068329  train_rmse:  0.010914764367043972 and val_rmse:  0.41556355357170105/n\n",
      "epoch 189 train_mae:  0.014476345852017403 and val_mae:  0.443070650100708  train_rmse:  0.011202259920537472 and val_rmse:  0.41585618257522583/n\n",
      "epoch 190 train_mae:  0.012937710620462894 and val_mae:  0.44307395815849304  train_rmse:  0.007180892396718264 and val_rmse:  0.41522863507270813/n\n",
      "epoch 191 train_mae:  0.013759234920144081 and val_mae:  0.4434417486190796  train_rmse:  0.01104887668043375 and val_rmse:  0.41568154096603394/n\n",
      "epoch 192 train_mae:  0.015542251989245415 and val_mae:  0.443310409784317  train_rmse:  0.012137985788285732 and val_rmse:  0.4154438376426697/n\n",
      "epoch 193 train_mae:  0.01350695826113224 and val_mae:  0.4433669149875641  train_rmse:  0.010381423868238926 and val_rmse:  0.41562899947166443/n\n",
      "epoch 194 train_mae:  0.014161995612084866 and val_mae:  0.4432116448879242  train_rmse:  0.01324242539703846 and val_rmse:  0.41558828949928284/n\n",
      "epoch 195 train_mae:  0.014163624495267868 and val_mae:  0.4433399438858032  train_rmse:  0.009901872836053371 and val_rmse:  0.4156453311443329/n\n",
      "epoch 196 train_mae:  0.014791691675782204 and val_mae:  0.4431523382663727  train_rmse:  0.020516082644462585 and val_rmse:  0.4153382480144501/n\n",
      "epoch 197 train_mae:  0.0142716309055686 and val_mae:  0.4429683983325958  train_rmse:  0.009372946806252003 and val_rmse:  0.4153882563114166/n\n",
      "epoch 198 train_mae:  0.01364091131836176 and val_mae:  0.44305509328842163  train_rmse:  0.007998769171535969 and val_rmse:  0.4152091145515442/n\n",
      "epoch 199 train_mae:  0.01282231230288744 and val_mae:  0.44321969151496887  train_rmse:  0.00786053016781807 and val_rmse:  0.414669930934906/n\n",
      "epoch 200 train_mae:  0.011132419109344482 and val_mae:  0.4431614875793457  train_rmse:  0.005643581040203571 and val_rmse:  0.41488397121429443/n\n",
      "epoch 201 train_mae:  0.011469386518001556 and val_mae:  0.443092942237854  train_rmse:  0.005359283648431301 and val_rmse:  0.4152410328388214/n\n",
      "epoch 202 train_mae:  0.012303769588470459 and val_mae:  0.4432177245616913  train_rmse:  0.006964004598557949 and val_rmse:  0.4155494272708893/n\n",
      "epoch 203 train_mae:  0.014481447637081146 and val_mae:  0.4431970417499542  train_rmse:  0.009560582228004932 and val_rmse:  0.4150358736515045/n\n",
      "epoch 204 train_mae:  0.01172917801886797 and val_mae:  0.4431977868080139  train_rmse:  0.0061341519467532635 and val_rmse:  0.4147355556488037/n\n",
      "epoch 205 train_mae:  0.013324382714927197 and val_mae:  0.4430212080478668  train_rmse:  0.010205769911408424 and val_rmse:  0.4150317907333374/n\n",
      "epoch 206 train_mae:  0.012297464534640312 and val_mae:  0.4431060254573822  train_rmse:  0.006289821118116379 and val_rmse:  0.4150371551513672/n\n",
      "epoch 207 train_mae:  0.011321713216602802 and val_mae:  0.44323018193244934  train_rmse:  0.00582223990932107 and val_rmse:  0.41512957215309143/n\n",
      "epoch 208 train_mae:  0.015217728912830353 and val_mae:  0.4430420398712158  train_rmse:  0.011016305536031723 and val_rmse:  0.4151652157306671/n\n",
      "epoch 209 train_mae:  0.012565787881612778 and val_mae:  0.44312784075737  train_rmse:  0.00805165059864521 and val_rmse:  0.41509124636650085/n\n",
      "epoch 210 train_mae:  0.013391760177910328 and val_mae:  0.44312500953674316  train_rmse:  0.011862769722938538 and val_rmse:  0.41569745540618896/n\n",
      "epoch 211 train_mae:  0.012507151812314987 and val_mae:  0.443057656288147  train_rmse:  0.008577404543757439 and val_rmse:  0.4149286150932312/n\n",
      "epoch 212 train_mae:  0.014858200214803219 and val_mae:  0.442942351102829  train_rmse:  0.012895004823803902 and val_rmse:  0.4150756001472473/n\n",
      "epoch 213 train_mae:  0.012732842937111855 and val_mae:  0.44330984354019165  train_rmse:  0.007921815849840641 and val_rmse:  0.4149172008037567/n\n",
      "epoch 214 train_mae:  0.01604924350976944 and val_mae:  0.44282472133636475  train_rmse:  0.015551727265119553 and val_rmse:  0.4149280786514282/n\n",
      "epoch 215 train_mae:  0.015380851924419403 and val_mae:  0.4430156350135803  train_rmse:  0.01318767573684454 and val_rmse:  0.414938747882843/n\n",
      "epoch 216 train_mae:  0.01314717810600996 and val_mae:  0.4431177079677582  train_rmse:  0.009965555742383003 and val_rmse:  0.4147008955478668/n\n",
      "epoch 217 train_mae:  0.011566688306629658 and val_mae:  0.44300657510757446  train_rmse:  0.006361427251249552 and val_rmse:  0.4146774113178253/n\n",
      "epoch 218 train_mae:  0.011894351802766323 and val_mae:  0.443001389503479  train_rmse:  0.006382546853274107 and val_rmse:  0.41505807638168335/n\n",
      "epoch 219 train_mae:  0.014857643283903599 and val_mae:  0.44290608167648315  train_rmse:  0.013622395694255829 and val_rmse:  0.41478466987609863/n\n",
      "epoch 220 train_mae:  0.013281329534947872 and val_mae:  0.44283246994018555  train_rmse:  0.008575163781642914 and val_rmse:  0.4148404002189636/n\n",
      "epoch 221 train_mae:  0.015034239739179611 and val_mae:  0.4429989755153656  train_rmse:  0.013365396298468113 and val_rmse:  0.4148082435131073/n\n",
      "epoch 222 train_mae:  0.013411414809525013 and val_mae:  0.442982941865921  train_rmse:  0.008124994114041328 and val_rmse:  0.41467973589897156/n\n",
      "epoch 223 train_mae:  0.012302941642701626 and val_mae:  0.4429919123649597  train_rmse:  0.007288194261491299 and val_rmse:  0.41501539945602417/n\n",
      "epoch 224 train_mae:  0.013289735652506351 and val_mae:  0.4429381191730499  train_rmse:  0.008184297941625118 and val_rmse:  0.4148142337799072/n\n",
      "epoch 225 train_mae:  0.014904097653925419 and val_mae:  0.44289007782936096  train_rmse:  0.00965692289173603 and val_rmse:  0.41474995017051697/n\n",
      "epoch 226 train_mae:  0.012508816085755825 and val_mae:  0.44283679127693176  train_rmse:  0.008069840259850025 and val_rmse:  0.41470789909362793/n\n",
      "epoch 227 train_mae:  0.01202470250427723 and val_mae:  0.44283413887023926  train_rmse:  0.007126916199922562 and val_rmse:  0.41470110416412354/n\n",
      "epoch 228 train_mae:  0.013612291775643826 and val_mae:  0.4429354965686798  train_rmse:  0.014729220420122147 and val_rmse:  0.41485294699668884/n\n",
      "epoch 229 train_mae:  0.010905894450843334 and val_mae:  0.44287213683128357  train_rmse:  0.006771508138626814 and val_rmse:  0.4150102436542511/n\n",
      "epoch 230 train_mae:  0.014336679130792618 and val_mae:  0.44289788603782654  train_rmse:  0.011560794897377491 and val_rmse:  0.41491204500198364/n\n",
      "epoch 231 train_mae:  0.012528670951724052 and val_mae:  0.44287997484207153  train_rmse:  0.008572935126721859 and val_rmse:  0.4148986339569092/n\n",
      "epoch 232 train_mae:  0.013763475231826305 and val_mae:  0.4428975284099579  train_rmse:  0.009531316347420216 and val_rmse:  0.41473308205604553/n\n",
      "epoch 233 train_mae:  0.011427435092628002 and val_mae:  0.44288352131843567  train_rmse:  0.006088152062147856 and val_rmse:  0.41485053300857544/n\n",
      "epoch 234 train_mae:  0.012205622158944607 and val_mae:  0.4429148733615875  train_rmse:  0.007699779700487852 and val_rmse:  0.4148583710193634/n\n",
      "epoch 235 train_mae:  0.01220126636326313 and val_mae:  0.4429258704185486  train_rmse:  0.006434096023440361 and val_rmse:  0.41477900743484497/n\n",
      "epoch 236 train_mae:  0.012485042214393616 and val_mae:  0.4428657293319702  train_rmse:  0.006908885668963194 and val_rmse:  0.41495004296302795/n\n",
      "epoch 237 train_mae:  0.0147422906011343 and val_mae:  0.4428480565547943  train_rmse:  0.013668579049408436 and val_rmse:  0.4147928059101105/n\n",
      "epoch 238 train_mae:  0.011511119082570076 and val_mae:  0.4428396224975586  train_rmse:  0.0065867239609360695 and val_rmse:  0.4148048162460327/n\n",
      "epoch 239 train_mae:  0.01612919382750988 and val_mae:  0.4429335594177246  train_rmse:  0.014453851617872715 and val_rmse:  0.41484394669532776/n\n",
      "epoch 240 train_mae:  0.013836042955517769 and val_mae:  0.44288256764411926  train_rmse:  0.009909257292747498 and val_rmse:  0.41472524404525757/n\n",
      "epoch 241 train_mae:  0.01504429616034031 and val_mae:  0.4429101347923279  train_rmse:  0.014791485853493214 and val_rmse:  0.4148953855037689/n\n",
      "epoch 242 train_mae:  0.016634110361337662 and val_mae:  0.4429004490375519  train_rmse:  0.023297851905226707 and val_rmse:  0.41491225361824036/n\n",
      "epoch 243 train_mae:  0.01376593392342329 and val_mae:  0.442948579788208  train_rmse:  0.009526656940579414 and val_rmse:  0.41484615206718445/n\n",
      "epoch 244 train_mae:  0.012545371428132057 and val_mae:  0.442970335483551  train_rmse:  0.007172423880547285 and val_rmse:  0.4147665500640869/n\n",
      "epoch 245 train_mae:  0.014782286249101162 and val_mae:  0.44288134574890137  train_rmse:  0.012307175435125828 and val_rmse:  0.4150080978870392/n\n",
      "epoch 246 train_mae:  0.015734126791357994 and val_mae:  0.4428890347480774  train_rmse:  0.018413512036204338 and val_rmse:  0.41501209139823914/n\n",
      "epoch 247 train_mae:  0.013743595220148563 and val_mae:  0.44288748502731323  train_rmse:  0.00945273693650961 and val_rmse:  0.41492316126823425/n\n",
      "epoch 248 train_mae:  0.012695484794676304 and val_mae:  0.4429019093513489  train_rmse:  0.008098114281892776 and val_rmse:  0.414852499961853/n\n",
      "epoch 249 train_mae:  0.01309007778763771 and val_mae:  0.4429357945919037  train_rmse:  0.008915185928344727 and val_rmse:  0.41475147008895874/n\n",
      "epoch 250 train_mae:  0.011503092013299465 and val_mae:  0.4429072141647339  train_rmse:  0.0062572406604886055 and val_rmse:  0.41480547189712524/n\n",
      "epoch 251 train_mae:  0.014761608093976974 and val_mae:  0.4429313838481903  train_rmse:  0.010594426654279232 and val_rmse:  0.41478824615478516/n\n",
      "epoch 252 train_mae:  0.013445965945720673 and val_mae:  0.4429246783256531  train_rmse:  0.009299399331212044 and val_rmse:  0.41478461027145386/n\n",
      "epoch 253 train_mae:  0.010824974626302719 and val_mae:  0.442958265542984  train_rmse:  0.004871163982897997 and val_rmse:  0.41475602984428406/n\n",
      "epoch 254 train_mae:  0.01200010720640421 and val_mae:  0.4429432451725006  train_rmse:  0.006296637933701277 and val_rmse:  0.414772629737854/n\n",
      "epoch 255 train_mae:  0.01347101666033268 and val_mae:  0.4429444968700409  train_rmse:  0.009966577403247356 and val_rmse:  0.4147930443286896/n\n",
      "epoch 256 train_mae:  0.012207116931676865 and val_mae:  0.44294026494026184  train_rmse:  0.007309987675398588 and val_rmse:  0.41475409269332886/n\n",
      "epoch 257 train_mae:  0.013407663442194462 and val_mae:  0.44294705986976624  train_rmse:  0.008265267126262188 and val_rmse:  0.414776474237442/n\n",
      "epoch 258 train_mae:  0.012011055834591389 and val_mae:  0.44291865825653076  train_rmse:  0.010623096488416195 and val_rmse:  0.41481831669807434/n\n",
      "epoch 259 train_mae:  0.012420504353940487 and val_mae:  0.4429053068161011  train_rmse:  0.00755772041156888 and val_rmse:  0.41475728154182434/n\n",
      "epoch 260 train_mae:  0.012889622710645199 and val_mae:  0.4429561495780945  train_rmse:  0.007243715692311525 and val_rmse:  0.414760023355484/n\n",
      "epoch 261 train_mae:  0.012275555171072483 and val_mae:  0.4429067075252533  train_rmse:  0.008234547451138496 and val_rmse:  0.4147000312805176/n\n",
      "epoch 262 train_mae:  0.013197534717619419 and val_mae:  0.4429143965244293  train_rmse:  0.007638024166226387 and val_rmse:  0.41475316882133484/n\n",
      "epoch 263 train_mae:  0.013052908703684807 and val_mae:  0.4429003596305847  train_rmse:  0.007641749456524849 and val_rmse:  0.4148026406764984/n\n",
      "epoch 264 train_mae:  0.01368115097284317 and val_mae:  0.4428922235965729  train_rmse:  0.009096586145460606 and val_rmse:  0.41478216648101807/n\n",
      "epoch 265 train_mae:  0.014667383395135403 and val_mae:  0.4429044723510742  train_rmse:  0.012734230607748032 and val_rmse:  0.41479817032814026/n\n",
      "epoch 266 train_mae:  0.012221311219036579 and val_mae:  0.44290071725845337  train_rmse:  0.008288365788757801 and val_rmse:  0.4147448241710663/n\n",
      "epoch 267 train_mae:  0.011625973507761955 and val_mae:  0.4429205358028412  train_rmse:  0.006176012102514505 and val_rmse:  0.41476672887802124/n\n",
      "epoch 268 train_mae:  0.012352915480732918 and val_mae:  0.44290077686309814  train_rmse:  0.00804911833256483 and val_rmse:  0.4147687256336212/n\n",
      "epoch 269 train_mae:  0.012386091984808445 and val_mae:  0.44288960099220276  train_rmse:  0.007025228347629309 and val_rmse:  0.4147765636444092/n\n",
      "epoch 270 train_mae:  0.012648669071495533 and val_mae:  0.44288626313209534  train_rmse:  0.013500583358108997 and val_rmse:  0.4147924482822418/n\n",
      "epoch 271 train_mae:  0.013118713162839413 and val_mae:  0.44288572669029236  train_rmse:  0.007653055712580681 and val_rmse:  0.4148028492927551/n\n",
      "epoch 272 train_mae:  0.014301199465990067 and val_mae:  0.44287851452827454  train_rmse:  0.010602932423353195 and val_rmse:  0.4147879481315613/n\n",
      "epoch 273 train_mae:  0.01339002326130867 and val_mae:  0.44288402795791626  train_rmse:  0.013927636668086052 and val_rmse:  0.41479089856147766/n\n",
      "epoch 274 train_mae:  0.015554893761873245 and val_mae:  0.442886620759964  train_rmse:  0.014177960343658924 and val_rmse:  0.4147670865058899/n\n",
      "epoch 275 train_mae:  0.014909437857568264 and val_mae:  0.44287097454071045  train_rmse:  0.015811940655112267 and val_rmse:  0.41473889350891113/n\n",
      "epoch 276 train_mae:  0.013733232393860817 and val_mae:  0.44287559390068054  train_rmse:  0.011200189590454102 and val_rmse:  0.4147682785987854/n\n",
      "epoch 277 train_mae:  0.011881465092301369 and val_mae:  0.44287821650505066  train_rmse:  0.0072770798578858376 and val_rmse:  0.41478753089904785/n\n",
      "epoch 278 train_mae:  0.013079972937703133 and val_mae:  0.4428700804710388  train_rmse:  0.009401985444128513 and val_rmse:  0.41477319598197937/n\n",
      "epoch 279 train_mae:  0.013446885161101818 and val_mae:  0.4428756535053253  train_rmse:  0.010079034604132175 and val_rmse:  0.41477590799331665/n\n",
      "epoch 280 train_mae:  0.013725182972848415 and val_mae:  0.4428769648075104  train_rmse:  0.010678849183022976 and val_rmse:  0.4147242605686188/n\n",
      "epoch 281 train_mae:  0.012962413020431995 and val_mae:  0.44287925958633423  train_rmse:  0.00820036418735981 and val_rmse:  0.41474154591560364/n\n",
      "epoch 282 train_mae:  0.013559368439018726 and val_mae:  0.44287416338920593  train_rmse:  0.012807768769562244 and val_rmse:  0.4147772789001465/n\n",
      "epoch 283 train_mae:  0.012798760086297989 and val_mae:  0.4428844749927521  train_rmse:  0.008084947243332863 and val_rmse:  0.41476359963417053/n\n",
      "epoch 284 train_mae:  0.013142656534910202 and val_mae:  0.44288185238838196  train_rmse:  0.008063646964728832 and val_rmse:  0.41474494338035583/n\n",
      "epoch 285 train_mae:  0.012197543866932392 and val_mae:  0.4428809583187103  train_rmse:  0.0069245584309101105 and val_rmse:  0.41476696729660034/n\n",
      "epoch 286 train_mae:  0.012303018011152744 and val_mae:  0.44288498163223267  train_rmse:  0.009451935067772865 and val_rmse:  0.4147636890411377/n\n",
      "epoch 287 train_mae:  0.012024401687085629 and val_mae:  0.4428758919239044  train_rmse:  0.0067440215498209 and val_rmse:  0.41473686695098877/n\n",
      "epoch 288 train_mae:  0.014543772675096989 and val_mae:  0.4428892135620117  train_rmse:  0.011659510433673859 and val_rmse:  0.41475462913513184/n\n",
      "epoch 289 train_mae:  0.013615391217172146 and val_mae:  0.4428867697715759  train_rmse:  0.00947206187993288 and val_rmse:  0.4147440493106842/n\n",
      "epoch 290 train_mae:  0.012406173162162304 and val_mae:  0.44288161396980286  train_rmse:  0.008200709708034992 and val_rmse:  0.41476354002952576/n\n",
      "epoch 291 train_mae:  0.01191046554595232 and val_mae:  0.4428805112838745  train_rmse:  0.008435513824224472 and val_rmse:  0.4147501289844513/n\n",
      "epoch 292 train_mae:  0.012043016031384468 and val_mae:  0.4428735673427582  train_rmse:  0.0071233054623007774 and val_rmse:  0.4147617816925049/n\n",
      "epoch 293 train_mae:  0.013018445111811161 and val_mae:  0.44287851452827454  train_rmse:  0.007382367271929979 and val_rmse:  0.41476893424987793/n\n",
      "epoch 294 train_mae:  0.013288474641740322 and val_mae:  0.442874938249588  train_rmse:  0.008281061425805092 and val_rmse:  0.41475409269332886/n\n",
      "epoch 295 train_mae:  0.012952703982591629 and val_mae:  0.4428766071796417  train_rmse:  0.00906788744032383 and val_rmse:  0.41476187109947205/n\n",
      "epoch 296 train_mae:  0.013153093867003918 and val_mae:  0.4428699016571045  train_rmse:  0.007599768228828907 and val_rmse:  0.41476666927337646/n\n",
      "epoch 297 train_mae:  0.016432905569672585 and val_mae:  0.4428768455982208  train_rmse:  0.01626775600016117 and val_rmse:  0.41476142406463623/n\n",
      "epoch 298 train_mae:  0.013964790850877762 and val_mae:  0.44287434220314026  train_rmse:  0.01369626447558403 and val_rmse:  0.41475164890289307/n\n",
      "epoch 299 train_mae:  0.013607964850962162 and val_mae:  0.4428691864013672  train_rmse:  0.011481653898954391 and val_rmse:  0.414752334356308/n\n",
      "epoch 300 train_mae:  0.012479184195399284 and val_mae:  0.4428718388080597  train_rmse:  0.00748932221904397 and val_rmse:  0.4147564470767975/n\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, loss, optimizer, n_epochs, device, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15d032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbede72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e9c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc8c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9a277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785c3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c058d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698537a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a164cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2032a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028c95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418140e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516957e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43056d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b43d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ce693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cd47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3553103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
